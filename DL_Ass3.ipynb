{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "hFpfnKVQDceU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L3GRHUEHoqKY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import pandas as pd\n",
        "import wandb\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA ANALYSIS AND PREPROCESSING"
      ],
      "metadata": {
        "id": "EsftGaY4DjMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_load(path):\n",
        "    # input - English\n",
        "    # output - മലയാളം (Malayalam)\n",
        "    df = pd.read_csv(path,header=None)\n",
        "    input_data = df[1].tolist()\n",
        "    output_data = df[0].tolist()\n",
        "    return input_data, output_data\n",
        "def create_char_set(train, val):\n",
        "    char_set = set()\n",
        "    for word in train:\n",
        "        for char in word:\n",
        "            char_set.add(char)\n",
        "    for word in val:\n",
        "        for char in word:\n",
        "            char_set.add(char)\n",
        "    return char_set\n"
      ],
      "metadata": {
        "id": "h979rUzLo7lJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input, train_output = data_load(\"/content/drive/MyDrive/DA6401_Assignment-3/dakshina_dataset_v1.0/dakshina_dataset_v1.0/ml/lexicons/output.csv\")\n",
        "val_input, val_output = data_load(\"/content/drive/MyDrive/DA6401_Assignment-3/dakshina_dataset_v1.0/dakshina_dataset_v1.0/ml/lexicons/output_2.csv\")\n",
        "test_input, test_output = data_load(\"/content/drive/MyDrive/DA6401_Assignment-3/dakshina_dataset_v1.0/dakshina_dataset_v1.0/ml/lexicons/output_1.csv\")\n",
        "print(\"Number of training samples: \", len(train_input))\n",
        "print(\"Number of validation samples: \", len(val_input))\n",
        "print(\"Number of test samples: \", len(test_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DW1yhmdo-WX",
        "outputId": "34b7e7fc-874f-4cc3-c6d3-603486cebf40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  58382\n",
            "Number of validation samples:  5641\n",
            "Number of test samples:  5610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_char_set(train, val):\n",
        "    char_set = set()\n",
        "    for word in train:\n",
        "        # Convert word to string to handle potential float values\n",
        "        for char in str(word):\n",
        "            char_set.add(char)\n",
        "    for word in val:\n",
        "        # Convert word to string to handle potential float values\n",
        "        for char in str(word):\n",
        "            char_set.add(char)\n",
        "    return char_set\n",
        "eng_chars = create_char_set(train_input, val_input)\n",
        "print(\"Total English characters: \",len(eng_chars))\n",
        "print(sorted(eng_chars))\n",
        "mal_chars = create_char_set(train_output, val_output)\n",
        "print(\"Total Malayalam characters: \",len(mal_chars))\n",
        "print(sorted(mal_chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrhxOk3B_DZE",
        "outputId": "234ffb29-bc28-4776-9276-0fd7bfcf8195"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total English characters:  26\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Total Malayalam characters:  71\n",
            "[' ', 'ം', 'ഃ', 'അ', 'ആ', 'ഇ', 'ഈ', 'ഉ', 'ഊ', 'ഋ', 'എ', 'ഏ', 'ഐ', 'ഒ', 'ഓ', 'ഔ', 'ക', 'ഖ', 'ഗ', 'ഘ', 'ങ', 'ച', 'ഛ', 'ജ', 'ഝ', 'ഞ', 'ട', 'ഠ', 'ഡ', 'ഢ', 'ണ', 'ത', 'ഥ', 'ദ', 'ധ', 'ന', 'പ', 'ഫ', 'ബ', 'ഭ', 'മ', 'യ', 'ര', 'റ', 'ല', 'ള', 'ഴ', 'വ', 'ശ', 'ഷ', 'സ', 'ഹ', 'ാ', 'ി', 'ീ', 'ു', 'ൂ', 'ൃ', 'െ', 'േ', 'ൈ', 'ൊ', 'ോ', '്', 'ൗ', 'ൺ', 'ൻ', 'ർ', 'ൽ', 'ൾ', '\\u200c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_eng = len(max((str(word) for word in train_input + val_input + test_input), key=len))\n",
        "max_seq_mal = len(max((str(word) for word in train_output + val_output + test_output), key=len))\n",
        "print(\"Length of the longest English word in corpus:\",max_seq_eng)\n",
        "print(\"Length of the longest Malayalam word in corpus::\",max_seq_mal)\n",
        "#max_seq_eng = len(max(train_input+val_input+test_input, key=len))\n",
        "#max_seq_mal = len(max(train_output+val_output+test_output, key=len))\n",
        "#print(\"Length of the longest English word in corpus:\",max_seq_eng)\n",
        "#print(\"Length of the longest Malayalam word in corpus::\",max_seq_mal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juPpDWZe_HiY",
        "outputId": "fe88465d-de51-4af4-a2c7-dc55c91eda58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the longest English word in corpus: 32\n",
            "Length of the longest Malayalam word in corpus:: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_chars_idx = {char: idx + 3 for idx, char in enumerate(sorted(eng_chars))}\n",
        "eng_chars_idx['0'] = 0 # padding\n",
        "eng_chars_idx['\\t'] = 1 # <SOW>\n",
        "eng_chars_idx['\\n'] = 2 # <EOW>\n",
        "print(eng_chars_idx)\n",
        "mal_chars_idx = {char: idx+3 for idx, char in enumerate(sorted(mal_chars))}\n",
        "mal_chars_idx['0'] = 0 # padding\n",
        "mal_chars_idx['\\t'] = 1 # <SOW>\n",
        "mal_chars_idx['\\n'] = 2 # <EOW>\n",
        "print(mal_chars_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6TRgDAA_Kal",
        "outputId": "0ad93ed5-fa0f-4bc2-f44a-caa1a33d0cae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, '0': 0, '\\t': 1, '\\n': 2}\n",
            "{' ': 3, 'ം': 4, 'ഃ': 5, 'അ': 6, 'ആ': 7, 'ഇ': 8, 'ഈ': 9, 'ഉ': 10, 'ഊ': 11, 'ഋ': 12, 'എ': 13, 'ഏ': 14, 'ഐ': 15, 'ഒ': 16, 'ഓ': 17, 'ഔ': 18, 'ക': 19, 'ഖ': 20, 'ഗ': 21, 'ഘ': 22, 'ങ': 23, 'ച': 24, 'ഛ': 25, 'ജ': 26, 'ഝ': 27, 'ഞ': 28, 'ട': 29, 'ഠ': 30, 'ഡ': 31, 'ഢ': 32, 'ണ': 33, 'ത': 34, 'ഥ': 35, 'ദ': 36, 'ധ': 37, 'ന': 38, 'പ': 39, 'ഫ': 40, 'ബ': 41, 'ഭ': 42, 'മ': 43, 'യ': 44, 'ര': 45, 'റ': 46, 'ല': 47, 'ള': 48, 'ഴ': 49, 'വ': 50, 'ശ': 51, 'ഷ': 52, 'സ': 53, 'ഹ': 54, 'ാ': 55, 'ി': 56, 'ീ': 57, 'ു': 58, 'ൂ': 59, 'ൃ': 60, 'െ': 61, 'േ': 62, 'ൈ': 63, 'ൊ': 64, 'ോ': 65, '്': 66, 'ൗ': 67, 'ൺ': 68, 'ൻ': 69, 'ർ': 70, 'ൽ': 71, 'ൾ': 72, '\\u200c': 73, '0': 0, '\\t': 1, '\\n': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx2char_mal = {idx: char for char, idx in mal_chars_idx.items()}\n",
        "mal_embedd_size = 29\n",
        "eng_embedd_size = 32\n"
      ],
      "metadata": {
        "id": "pVIA5eyJ_dul"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocess(data, max_seq, chars_idx):\n",
        "    # Add start & end tokens and padding\n",
        "    # sow = \"\\t\" & eow = \"\\n\"\n",
        "    sow = \"\\t\"\n",
        "    eow = \"\\n\"\n",
        "    # Explicitly convert word to string before concatenation\n",
        "    padded_data = [sow + str(word) + \"0\" * (max_seq - len(str(word))) + eow for word in data]\n",
        "    # Convert sequences to indices\n",
        "    seq2idx = torch.LongTensor([[chars_idx[char] for char in seq] for seq in padded_data])\n",
        "    return seq2idx"
      ],
      "metadata": {
        "id": "61WajEeRQLk_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx_eng = data_preprocess(train_input, max_seq_eng, eng_chars_idx)\n",
        "train_idx_mal = data_preprocess(train_output, max_seq_mal, mal_chars_idx)\n",
        "val_idx_eng = data_preprocess(val_input, max_seq_eng, eng_chars_idx)\n",
        "val_idx_mal = data_preprocess(val_output, max_seq_mal, mal_chars_idx)\n",
        "test_idx_eng = data_preprocess(test_input, max_seq_eng, eng_chars_idx)\n",
        "test_idx_mal = data_preprocess(test_output, max_seq_mal, mal_chars_idx)"
      ],
      "metadata": {
        "id": "4HkEwir9DOts"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "class Dataset():\n",
        "    def __init__(self, train_idx_src, train_idx_tgt):\n",
        "        self.train_idx_src = train_idx_src\n",
        "        self.train_idx_tgt = train_idx_tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_idx_src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sample = self.train_idx_src[idx]\n",
        "        tgt_label = self.train_idx_tgt[idx]\n",
        "        return src_sample, tgt_label\n",
        "\n",
        "# Assuming train_idx_src and train_idx_tgt are lists or arrays\n",
        "train_dataset = Dataset(train_idx_eng, train_idx_mal)\n",
        "val_dataset = Dataset(val_idx_eng, val_idx_mal)\n",
        "test_dataset = Dataset(test_idx_eng, test_idx_mal)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "gaCQWoR-_oJx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder part"
      ],
      "metadata": {
        "id": "HIGeEmcU_rEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder part\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim = 32,\n",
        "                 emb_dim = 256,\n",
        "                 enc_hid_dim = 256,\n",
        "                 cell_type='gru',\n",
        "                 num_layers=2,\n",
        "                 dropout = 0,\n",
        "                 #bidirectional = True # Keep commented out for the non-attention sweep\n",
        "                 ):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.num_layers = num_layers\n",
        "        # Embedding the input\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.cell_type = cell_type\n",
        "        # Add dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Bidirectional part is commented out, so assume unidirectional (1 direction)\n",
        "        self.val_direction = 1 # Initialize val_direction unconditionally to 1\n",
        "\n",
        "        # If you uncomment bidirectional later, you'll need to modify this logic\n",
        "        # if bidirectional:\n",
        "        #     self.val_direction = 2\n",
        "        # else :\n",
        "        #     self.val_direction = 1\n",
        "\n",
        "\n",
        "        if cell_type.lower() == 'rnn':\n",
        "            self.rnn = nn.RNN(input_size = emb_dim,\n",
        "                              hidden_size = enc_hid_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout = dropout,\n",
        "                              #bidirectional= False, # Should be False if not a param\n",
        "                              batch_first=True)\n",
        "        elif cell_type.lower() == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_size = emb_dim,\n",
        "                              hidden_size = enc_hid_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout = dropout,\n",
        "                              #bidirectional= False, # Should be False if not a param\n",
        "                              batch_first=True)\n",
        "        elif cell_type.lower() == 'gru':\n",
        "            self.rnn = nn.GRU(input_size = emb_dim,\n",
        "                              hidden_size = enc_hid_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout = dropout,\n",
        "                              #bidirectional= False, # Should be False if not a param\n",
        "                              batch_first=True)\n",
        "\n",
        "    def forward(self, src, hidden, cell=None):\n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        if self.cell_type == 'lstm':\n",
        "            # nn.LSTM returns output, (h_n, c_n)\n",
        "            output,(hidden,cell) = self.rnn(embedded, (hidden,cell))\n",
        "        else:\n",
        "            # nn.RNN and nn.GRU return output, h_n\n",
        "            output, hidden = self.rnn(embedded, hidden)\n",
        "            cell = None # Ensure cell is None when not LSTM\n",
        "\n",
        "        # output shape is (batch_size, seq_len, hidden_size * num_directions) if batch_first=True\n",
        "        # hidden/cell shapes are (num_layers * num_directions, batch_size, hidden_size)\n",
        "        return output, hidden, cell"
      ],
      "metadata": {
        "id": "5eopr6qd_qqw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder part"
      ],
      "metadata": {
        "id": "8_rD6e9H_vOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder part\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim = 29,\n",
        "                 emb_dim = 256,\n",
        "                 dec_hid_dim = 256,\n",
        "                 cell_type='gru',\n",
        "                 num_layers=2,\n",
        "                 dropout = 0,\n",
        "                 #bidirectional = True, # Keep commented if not using bidirectional\n",
        "                 attention = False,\n",
        "                 attention_dim = None\n",
        "                 ):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Embedding part\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.attention = attention\n",
        "        # Dropout to add onto embedded input\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.cell_type = cell_type\n",
        "\n",
        "        # Define val_direction - assuming not using bidirectional decoder\n",
        "        self.val_direction = 1\n",
        "\n",
        "        #if bidirectional :\n",
        "            #self.val_direction = 2\n",
        "        #else :\n",
        "           # self.val_direction = 1\n",
        "\n",
        "        # Linear layer to get the output\n",
        "        self.W1 = nn.Linear(dec_hid_dim * self.val_direction, output_dim)\n",
        "        # Softmax layer\n",
        "        self.softmax = F.softmax # This is not used in the forward pass when CrossEntropyLoss is used\n",
        "\n",
        "        if attention:\n",
        "            self.attention_dim = attention_dim\n",
        "            # Need to define self.input_size and self.hidden_dimension etc if using attention\n",
        "            # Assuming input_size is emb_dim and hidden_dimension is dec_hid_dim for this context\n",
        "            self.input_size = emb_dim # Example definition, adjust if needed\n",
        "            self.hidden_dimension = dec_hid_dim # Example definition, adjust if needed\n",
        "            self.attention_out_dimension = self.hidden_dimension # Example definition, adjust if needed\n",
        "            # If attention is concatenated to the input embedding, the input size to the RNN/GRU changes\n",
        "            # This input size definition was likely for the attention calculation itself, not the RNN input.\n",
        "            # The input to the RNN/GRU should be emb_dim + context_vector_dim if attention is concatenated to input.\n",
        "            # Or, if attention is used to modify the hidden state, the input size is just emb_dim.\n",
        "            # Based on your Decoder forward, attention output (context) is concatenated to the embedded output.\n",
        "            # So, if attention, the input size to RNN/GRU should be emb_dim + attention_out_dimension (which seems to be dec_hid_dim).\n",
        "            rnn_input_size = emb_dim\n",
        "            if attention:\n",
        "                 # Assuming context vector size is dec_hid_dim * self.val_direction if derived from encoder_outputs\n",
        "                 # based on calculate_attention logic and the linear layer in Seq2Seq.\n",
        "                 # Let's check calculate_attention function again.\n",
        "                 # The calculate_attention function returns normalized_context_vector, which is (batch_size, 1, hidden_size*num_directions) if encoder_outputs are (batch_size, seq_len, hidden_size*num_directions)\n",
        "                 # Based on the code, encoder_outputs are (batch_size, seq_len, enc_hid_dim * val_direction) from Encoder's batch_first output\n",
        "                 # calculate_attention permutes it to (seq_len, batch_size, ...), operates, and returns (batch_size, 1, hidden_size*num_directions)\n",
        "                 # So context vector size is enc_hid_dim * val_direction.\n",
        "                 # If concatenated, rnn_input_size = emb_dim + enc_hid_dim * val_direction\n",
        "                 rnn_input_size = emb_dim + self.encoder.enc_hid_dim * self.encoder.val_direction # Assuming encoder is accessible and has val_direction attribute\n",
        "\n",
        "            self.U = nn.Sequential(nn.Linear( self.hidden_dimension, self.hidden_dimension), nn.LeakyReLU())\n",
        "            self.W = nn.Sequential(nn.Linear( self.hidden_dimension, self.hidden_dimension), nn.LeakyReLU())\n",
        "            self.V = nn.Sequential(nn.Linear( self.hidden_dimension, self.attention_out_dimension), nn.LeakyReLU())\n",
        "\n",
        "\n",
        "        # Determine the input size for the RNN/GRU based on whether attention is used for concatenation\n",
        "        rnn_input_size = emb_dim\n",
        "        if attention:\n",
        "             # If context is concatenated to the input embedding before the RNN/GRU\n",
        "             # Need to get the size of the context vector from the calculate_attention logic\n",
        "             # Based on Seq2Seq and Decoder forward, context comes from encoder_outputs (batch_size, seq_len, enc_hid_dim * val_direction)\n",
        "             # calculate_attention output is (batch_size, 1, enc_hid_dim * val_direction)\n",
        "             # So context size is enc_hid_dim * val_direction\n",
        "             # Assuming encoder is an attribute of the Seq2Seq model, not directly available in Decoder init.\n",
        "             # You might need to pass encoder config or context size to the Decoder if attention is used.\n",
        "             # Let's assume for now the context size is enc_hid_dim * self.encoder.val_direction (if accessible)\n",
        "             # Or, if attention_dim is intended for the context size, use that.\n",
        "             # Let's use attention_dim for now, assuming that's the intended size of the context vector.\n",
        "             # However, based on the calculate_attention structure, the context vector size is tied to encoder output dim.\n",
        "             # Let's pass encoder_hid_dim and encoder_val_direction to Decoder init if attention is True.\n",
        "             # For now, let's assume attention_dim is the correct size to add.\n",
        "             rnn_input_size = emb_dim + self.attention_dim\n",
        "\n",
        "\n",
        "        if cell_type.lower() == 'rnn':\n",
        "            self.rnn = nn.RNN(input_size=rnn_input_size, # Use rnn_input_size\n",
        "                              hidden_size = dec_hid_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout = dropout,\n",
        "                              #bidirectional= bidirectional, # Keep commented if not using bidirectional\n",
        "                              batch_first=True)\n",
        "        elif cell_type.lower() == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_size=rnn_input_size, # Use rnn_input_size\n",
        "                               hidden_size = dec_hid_dim,\n",
        "                               num_layers=num_layers,\n",
        "                               dropout = dropout,\n",
        "                               #bidirectional= bidirectional, # Keep commented if not using bidirectional\n",
        "                               batch_first=True)\n",
        "        elif cell_type.lower() == 'gru':\n",
        "            self.rnn = nn.GRU(input_size=rnn_input_size, # Use rnn_input_size\n",
        "                              hidden_size = dec_hid_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout = dropout,\n",
        "                              #bidirectional= bidirectional, # Keep commented if not using bidirectional\n",
        "                              batch_first=True)\n",
        "\n",
        "        # self.fc_out = nn.Linear(dec_hid_dim, output_dim) # This layer is not used in the forward pass\n",
        "\n",
        "\n",
        "    # The calculate_attention function needs to be a method of the class if it uses self attributes, or a standalone function\n",
        "    # If it's a method, it needs 'self' as the first parameter.\n",
        "    # Based on its structure, it seems intended to be a standalone helper function or a method needing more context (like encoder_outputs structure)\n",
        "    # Let's move it outside the class definition for now, assuming it's a helper.\n",
        "    # Or, more likely, it should be integrated into the Decoder forward method directly if it's simple attention.\n",
        "    # If it's a complex attention module, it should be a separate nn.Module.\n",
        "    # Given it's defined *inside* the Decoder class but without 'self', it's syntactically incorrect.\n",
        "    # Let's remove the misplaced function definition for now. If attention is needed, it must be correctly implemented.\n",
        "    # Assuming for the \"no attention\" sweep this function isn't called anyway.\n",
        "\n",
        "    # Removed the misplaced calculate_attention function definition here\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, cell=None,encoder_outputs=None):\n",
        "#         Incorporate dropout in embedding.\n",
        "        # input shape: (batch_size, 1) if processing one token at a time\n",
        "        embedded = self.embedding(input) # embedded shape: (batch_size, 1, emb_dim)\n",
        "        output = self.dropout(embedded) # output shape: (batch_size, 1, emb_dim)\n",
        "\n",
        "        attention_weights = None\n",
        "#         If we are using attention, then we need to concatenate the context vector, which we obtain from attention\n",
        "\n",
        "        if self.attention and encoder_outputs is not None:\n",
        "            # The calculate_attention function needs to be defined correctly.\n",
        "            # Assuming the helper function calculate_attention is defined globally or imported.\n",
        "            # Need to ensure hidden and encoder_outputs shapes match what calculate_attention expects.\n",
        "            # Based on calculate_attention's assumed signature: calculate_attention(hidden, encoder_outputs)\n",
        "            # hidden shape (from Seq2Seq): (num_layers, batch_size, hidden_size) -> Needs permuting/reshaping for calculate_attention?\n",
        "            # encoder_outputs shape (from Seq2Seq): (batch_size, seq_len, hidden_size * num_directions) -> Needs permuting/reshaping for calculate_attention?\n",
        "            # Let's assume for now that the calculate_attention function (wherever it is) expects:\n",
        "            # hidden: (batch_size, hidden_size) - the hidden state for the *current* time step (single layer, single direction)\n",
        "            # encoder_outputs: (batch_size, src_seq_len, enc_hidden_size * enc_directions)\n",
        "            # This implies significant reshaping and potentially selecting the last layer's hidden state before passing to attention.\n",
        "            # However, the current Decoder forward is processing one token at a time. The 'hidden' state passed is the full state (num_layers, batch_size, hidden_size).\n",
        "            # If using Bahdanau/Luong style attention, the hidden state from the *previous* decoder step is used with encoder outputs.\n",
        "            # Let's assume `calculate_attention` is meant to take the current decoder hidden state `hidden` (possibly reshaped or last layer)\n",
        "            # and `encoder_outputs`. The `context` returned should be `(batch_size, 1, context_size)`.\n",
        "\n",
        "            # Example call assuming calculate_attention expects (batch_size, dec_hid_dim) and (batch_size, src_len, enc_hid_dim * enc_directions)\n",
        "            # This requires getting the appropriate hidden state from the full 'hidden' tensor.\n",
        "            # For a single-layer decoder, hidden is (1, batch_size, dec_hid_dim). For multi-layer, might use top layer.\n",
        "            # Let's simplify for now and assume a simple attention mechanism that might require reshaping.\n",
        "            # If `calculate_attention` was meant to be a simple dot product or similar, it should operate on appropriate shapes.\n",
        "            # Let's stick to fixing the RNN type error first and note the attention part needs proper implementation later.\n",
        "\n",
        "            # Assuming calculate_attention is corrected and returns (batch_size, 1, context_size)\n",
        "            # context, attention_weights = calculate_attention(...) # Call the corrected attention function\n",
        "            # output = torch.cat((output, context), 2) # Concatenate context to the input embedding\n",
        "\n",
        "\n",
        "            # **Temporarily skip attention logic for fixing the RNN type error**\n",
        "            pass # Do nothing if attention is False\n",
        "\n",
        "        # output shape is now (batch_size, 1, rnn_input_size) - where rnn_input_size = emb_dim or emb_dim + context_size\n",
        "\n",
        "\n",
        "        if self.cell_type == 'lstm':\n",
        "            # self.rnn is nn.LSTM, expects (input, (h_0, c_0))\n",
        "            rnn_output, (hidden, cell) = self.rnn(output, (hidden, cell)) # Pass tuple for LSTM\n",
        "        else: # Covers 'rnn' and 'gru'\n",
        "            # self.rnn is nn.RNN or nn.GRU, expects (input, h_0)\n",
        "            rnn_output, hidden = self.rnn(output, hidden) # Pass single tensor for RNN/GRU\n",
        "            cell = None # Ensure cell is None when not LSTM\n",
        "\n",
        "        # rnn_output shape is (batch_size, 1, dec_hid_dim * val_direction)\n",
        "\n",
        "        # Apply the final linear layer to get logits\n",
        "        # Squeeze the sequence length dimension (size 1) before the linear layer\n",
        "        output_logits = self.W1(rnn_output.squeeze(1)) # output_logits shape: (batch_size, output_dim)\n",
        "\n",
        "\n",
        "        return output_logits, hidden, cell, attention_weights # Return logits, updated states, attention weights\n",
        "\n",
        "# Moved calculate_attention outside the class or defined elsewhere if needed\n",
        "# def calculate_attention(hidden, encoder_outputs, ...):\n",
        "#     # ... implementation ...\n",
        "#     return context, attention_weights\n",
        "\n",
        "\n",
        "# The train1 and calc_test_acc functions should be the corrected versions from the previous turn.\n",
        "# They should obtain vocab_size using model.decoder.W1.out_features\n",
        "# and use reshape(-1, vocab_size) and reshape(-1) for loss calculation,\n",
        "# and torch.argmax(outputs, dim=2) for accuracy calculation."
      ],
      "metadata": {
        "id": "lUWK47xRSV0F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2Seq Model"
      ],
      "metadata": {
        "id": "tq3yw2D0_1m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq2Seq Model\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 decoder,\n",
        "                 # dec_inp_dim = 29, # This parameter is not strictly needed if we get output_dim from decoder\n",
        "                 enc_hid_dim = 256, # Parameter needed for linear transformation layers\n",
        "                 dec_hid_dim =256, # Parameter needed for linear transformation layers\n",
        "                 #bidirectional = True, # Keep commented for now\n",
        "                 enc_num_layers = 3, # Parameter needed for linear transformation layers\n",
        "                 dec_num_layers = 2, # Parameter needed for linear transformation layers\n",
        "                 cell_type = 'lstm', # Parameter needed for conditional logic and state transformation\n",
        "                 dropout = 0.2, # Parameter potentially needed elsewhere in Seq2Seq (though typically in Encoder/Decoder)\n",
        "                 attention = False # Parameter needed to control attention logic\n",
        "                ):\n",
        "\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Decoder input dimension\n",
        "        # self.dec_inp_dim = dec_inp_dim # Removed as it's not used for predictions tensor size\n",
        "\n",
        "        # Store the parameters needed for state transformation layers\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.enc_num_layers = enc_num_layers\n",
        "        self.dec_num_layers = dec_num_layers\n",
        "        self.cell_type = cell_type\n",
        "        self.dropout = dropout # Storing, but not directly used in Seq2Seq forward here\n",
        "        self.attention = attention # Storing, used in Seq2Seq forward\n",
        "\n",
        "        # Initialize val_direction (assuming unidirectional if bidirectional is commented)\n",
        "        self.val_direction = 1\n",
        "\n",
        "        self.softmax = F.softmax # Not used in forward with CrossEntropyLoss\n",
        "\n",
        "        # If attention is used, then we need to transform encoder's last hidden to decoder's first hidden\n",
        "        # Correct the input dimension for the linear transformation based on the flattened encoder hidden state\n",
        "        # Input size: num_layers * num_directions * hidden_size of encoder\n",
        "        # Output size: num_layers * num_directions * hidden_size of decoder\n",
        "        self.enc_dec_linear1 = nn.Linear(self.enc_hid_dim * self.val_direction * self.enc_num_layers,\n",
        "                                         self.dec_hid_dim * self.val_direction * self.dec_num_layers)\n",
        "\n",
        "\n",
        "        # Linear layer to transform encoder's last cell to decoder's first cell (only for LSTM)\n",
        "        if self.cell_type == 'lstm':\n",
        "             # Correct the input dimension for the linear transformation based on the flattened encoder cell state\n",
        "             self.enc_dec_cell_linear1 = nn.Linear(self.enc_hid_dim * self.val_direction * self.enc_num_layers,\n",
        "                                                  self.dec_hid_dim * self.val_direction * self.dec_num_layers)\n",
        "\n",
        "\n",
        "        # Get the global max_seq_mal value for target sequence length\n",
        "        global max_seq_mal\n",
        "        self.target_seq_len = max_seq_mal + 2 # Store the correct target sequence length\n",
        "\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing = False, is_training=False):\n",
        "        batch_size = source.shape[0]\n",
        "\n",
        "        # Initialize initial states for the encoder\n",
        "        # Correct shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        encoder_initial_hidden = torch.zeros(self.encoder.num_layers * self.encoder.val_direction,\n",
        "                                             batch_size,\n",
        "                                             self.encoder.enc_hid_dim,\n",
        "                                             device=device)\n",
        "        encoder_initial_cell = torch.zeros(self.encoder.num_layers * self.encoder.val_direction,\n",
        "                                           batch_size,\n",
        "                                           self.encoder.enc_hid_dim,\n",
        "                                           device=device) if self.encoder.cell_type == 'lstm' else None # Use encoder's cell_type\n",
        "\n",
        "\n",
        "        # Pass the full source sequence through the encoder\n",
        "        # encoder_output: (batch_size, seq_len, hidden_size * num_directions) if batch_first=True\n",
        "        # last_state: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        # cell_state: (num_layers * num_directions, batch_size, hidden_size) for LSTM\n",
        "        encoder_output, last_state, cell_state = self.encoder(source, encoder_initial_hidden, encoder_initial_cell)\n",
        "\n",
        "\n",
        "        # If attention is used, `encoder_outputs` for attention are the outputs from the encoder at each time step\n",
        "        if self.attention:\n",
        "             encoder_outputs = encoder_output # Shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        else:\n",
        "             encoder_outputs = None # Explicitly None if attention is off\n",
        "\n",
        "\n",
        "        # Encoder's last state is decoders first state (after transformation)\n",
        "        # last_state is (enc_num_layers * val_direction, batch_size, enc_hid_dim)\n",
        "\n",
        "        # Transform encoder's last hidden state to decoder's first hidden state\n",
        "        # Use the last hidden state from the encoder across all layers if applicable\n",
        "        # Reshape last_state to (batch_size, enc_num_layers * val_direction * enc_hid_dim) before linear transformation\n",
        "        last_state_reshaped = last_state.permute(1, 0, 2).reshape(batch_size, -1) # Shape: (batch_size, enc_num_layers * val_direction * enc_hid_dim)\n",
        "\n",
        "        # Apply the linear transformation\n",
        "        decoder_hidden_reshaped = self.enc_dec_linear1(last_state_reshaped) # Shape: (batch_size, dec_num_layers * val_direction * dec_hid_dim)\n",
        "\n",
        "        # Reshape back to (dec_num_layers * val_direction, batch_size, dec_hid_dim) for the decoder\n",
        "        # Note: The reshape should match the target shape (num_layers * num_directions, batch_size, hidden_size)\n",
        "        decoder_hidden = decoder_hidden_reshaped.reshape(batch_size, self.dec_num_layers * self.val_direction, self.dec_hid_dim).permute(1, 0, 2) # Shape: (dec_num_layers * val_direction, batch_size, dec_hid_dim)\n",
        "\n",
        "\n",
        "        # Here also, encoders last cell is decoders first cell, also transform to same dimension (for LSTM)\n",
        "        if  self.cell_type == 'lstm': # Use Seq2Seq's cell_type for this transformation\n",
        "            cell_state_reshaped = cell_state.permute(1, 0, 2).reshape(batch_size, -1) # Shape: (batch_size, enc_num_layers * val_direction * enc_hid_dim)\n",
        "            decoder_cell_reshaped = self.enc_dec_cell_linear1(cell_state_reshaped) # Shape: (batch_size, dec_num_layers * val_direction * dec_hid_dim)\n",
        "            # Reshape back to (dec_num_layers * val_direction, batch_size, dec_hid_dim) for the decoder\n",
        "            decoder_cell_state = decoder_cell_reshaped.reshape(batch_size, self.dec_num_layers * self.val_direction, self.dec_hid_dim).permute(1, 0, 2) # Shape: (dec_num_layers * val_direction, batch_size, dec_hid_dim)\n",
        "        else:\n",
        "            decoder_cell_state = None # Ensure cell_state is None for RNN/GRU\n",
        "\n",
        "\n",
        "        # Initialize predictions and attention_weights\n",
        "        # Get the correct output dimension from the decoder's final linear layer (assuming W1 is the final layer)\n",
        "        target_output_dim = self.decoder.W1.out_features # Use W1 as it's used in decoder forward\n",
        "\n",
        "        # Use the stored target sequence length\n",
        "        predictions = torch.zeros(batch_size, self.target_seq_len, target_output_dim, device = device)\n",
        "\n",
        "        # Attention weights shape: (batch_size, target_seq_len, source_seq_len)\n",
        "        # Need global max_seq_eng here\n",
        "        global max_seq_eng\n",
        "        attention_weights = torch.zeros(batch_size, self.target_seq_len, max_seq_eng + 2, device = device) if self.attention else None # Source seq len is max_seq_eng + SOW + EOW\n",
        "\n",
        "        # Initialize the first input to the decoder. This should be the <SOW> token (index 1).\n",
        "        # Use mal_chars_idx['\\t'] for the SOW index\n",
        "        decoder_input = torch.full((batch_size, 1), mal_chars_idx['\\t'], dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "        # Do decoding by char by char fashion by batch\n",
        "        # The loop should run self.target_seq_len times\n",
        "        for t in range(self.target_seq_len): # Loop over the correct target sequence length\n",
        "\n",
        "            # Pass the current hidden state and cell state (if LSTM) to the decoder\n",
        "            decoder_output, decoder_hidden, cell_state, attention_wts = self.decoder(\n",
        "                decoder_input,\n",
        "                decoder_hidden, # Pass the updated hidden state from the previous step\n",
        "                cell_state, # Pass the updated cell state from the previous step (will be None for RNN/GRU)\n",
        "                encoder_outputs # Pass encoder outputs for attention\n",
        "            )\n",
        "\n",
        "            # Store the prediction (logits) for the current time step\n",
        "            # decoder_output shape from Decoder forward: (batch_size, output_dim) after squeeze(1)\n",
        "            predictions[:, t, :] = decoder_output # decoder_output is already squeezed in Decoder forward\n",
        "\n",
        "\n",
        "            if self.attention and attention_wts is not None:\n",
        "                # Store attention weights if attention is used\n",
        "                # attention_wts shape from calculate_attention: (batch_size, source_seq_len)\n",
        "                attention_weights[:, t, :] = attention_wts # Store attention weights for this decoding step\n",
        "\n",
        "\n",
        "            # Determine the input for the next time step\n",
        "            # Teacher forcing should only happen if t is within the bounds of the target sequence\n",
        "            if teacher_forcing and is_training and t < self.target_seq_len - 1:\n",
        "                # Teacher forcing: use the actual target token as input to the decoder\n",
        "                # Target shape is (batch_size, target_seq_len). Input needs to be (batch_size, 1).\n",
        "                decoder_input = target[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                # Without teacher forcing or during inference: use the decoder's predicted token from the current time step as input for the next step\n",
        "                # Get the predicted token index (argmax) from the logits\n",
        "                predicted_token = torch.argmax(decoder_output, dim=-1) # shape: (batch_size)\n",
        "                decoder_input = predicted_token.unsqueeze(1).detach() # shape: (batch_size, 1), Detach from graph for the next input\n",
        "\n",
        "\n",
        "        # Return predictions and attention weights\n",
        "        # predictions shape: (batch_size, target_seq_len, output_dim)\n",
        "        # attention_weights shape: (batch_size, target_seq_len, source_seq_len)\n",
        "        return predictions, attention_weights"
      ],
      "metadata": {
        "id": "qoueRDUmT4vP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Accuracy"
      ],
      "metadata": {
        "id": "Qh5whKSk_6zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Accuracy\n",
        "def train1(model, train_loader, val_loader, epochs):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    # Use Ignore Index for padding token if needed, but 0 is padding and shouldn't be in target labels usually\n",
        "    # If padding token 0 should not contribute to loss/accuracy, use ignore_index=0 in CrossEntropyLoss\n",
        "    # Assuming padding token 0 *is* part of the target sequence for fixed length.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "\n",
        "    # Get the vocabulary size from the model's decoder's output layer\n",
        "    vocab_size = model.decoder.W1.out_features # Assuming W1 is the final output linear layer\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                data_loader = train_loader\n",
        "                model.train() # Set model to training mode\n",
        "            else:\n",
        "                data_loader = val_loader\n",
        "                model.eval() # Set model to evaluation mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total_tokens = 0\n",
        "\n",
        "\n",
        "            for inputs, labels in data_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device) # labels shape: (batch_size, target_seq_len)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # outputs shape: (batch_size, target_seq_len, vocab_size)\n",
        "                    outputs, _ = model(inputs, labels, epoch < epochs/2, phase == 'train')\n",
        "\n",
        "                    # Reshape outputs for CrossEntropyLoss\n",
        "                    # Expected shape for CrossEntropyLoss: (N, C) and (N) where C is vocab_size\n",
        "                    # outputs_reshaped: (batch_size * target_seq_len, vocab_size)\n",
        "                    outputs_reshaped = outputs.reshape(-1, vocab_size)\n",
        "\n",
        "                    # Reshape labels for CrossEntropyLoss\n",
        "                    # labels_reshaped: (batch_size * target_seq_len)\n",
        "                    labels_reshaped = labels.reshape(-1)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = criterion(outputs_reshaped, labels_reshaped)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "                        optimizer.step()\n",
        "\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0) # Scale loss by batch size\n",
        "\n",
        "                # Calculate accuracy - comparing predicted token indices with target token indices\n",
        "                # preds shape: (batch_size, target_seq_len)\n",
        "                preds = torch.argmax(outputs, dim=2) # Get the index of the highest logit for each token\n",
        "\n",
        "                # Count correct predictions\n",
        "                running_corrects += torch.sum(preds == labels).item() # Count total correctly predicted tokens in the batch\n",
        "                total_tokens += labels.numel() # Count total tokens in the batch (batch_size * target_seq_len)\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / len(data_loader.dataset) # Average loss per sample\n",
        "            epoch_acc = running_corrects / total_tokens # Accuracy across all tokens in the epoch\n",
        "\n",
        "            print(f'Epoch no: {epoch}')\n",
        "            if phase == 'train':\n",
        "                print(f'Train loss: {epoch_loss:.4f} \\t Train Accuracy: {epoch_acc:.4f}')\n",
        "                wandb.log({ 'Epoch': epoch, 'train_accuracy': epoch_acc * 100})\n",
        "                wandb.log({ 'Epoch': epoch, 'train_loss': epoch_loss})\n",
        "            else:\n",
        "                print(f'Validation loss: {epoch_loss:.4f} \\t Validation Accuracy: {epoch_acc:.4f}')\n",
        "                wandb.log({ 'Epoch': epoch, 'validation_accuracy': epoch_acc * 100})\n",
        "                wandb.log({ 'Epoch': epoch, 'validation_loss': epoch_loss})\n",
        "\n",
        "\n",
        "# Test Accuracy calculation\n",
        "# Assuming compute_score is not available or needed for token-wise accuracy\n",
        "def calc_test_acc(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    running_corrects = 0\n",
        "    total_tokens = 0\n",
        "    criterion = nn.CrossEntropyLoss() # Using CrossEntropyLoss directly on logits\n",
        "\n",
        "    # Get the vocabulary size from the model's decoder's output layer\n",
        "    vocab_size = model.decoder.W1.out_features # Assuming W1 is the final output linear layer\n",
        "\n",
        "\n",
        "    with torch.no_grad(): # No gradient calculation during evaluation\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            # outputs shape: (batch_size, target_seq_len, vocab_size)\n",
        "            # Pass target=None and teacher_forcing=False, is_training=False for inference\n",
        "            outputs, _ = model(inputs, None, False, False)\n",
        "\n",
        "            # Reshape outputs for loss calculation\n",
        "            outputs_reshaped = outputs.reshape(-1, vocab_size)\n",
        "\n",
        "            # Reshape targets for loss calculation\n",
        "            targets_reshaped = targets.reshape(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs_reshaped, targets_reshaped)\n",
        "            total_loss += loss.item() * inputs.size(0) # Scale loss by batch size\n",
        "\n",
        "            # Calculate accuracy - comparing predicted token indices with target token indices\n",
        "            # preds shape: (batch_size, target_seq_len)\n",
        "            preds = torch.argmax(outputs, dim=2) # Get the index of the highest logit for each token\n",
        "\n",
        "            # Count correct predictions\n",
        "            running_corrects += torch.sum(preds == targets).item() # Count total correctly predicted tokens in the batch\n",
        "            total_tokens += targets.numel() # Count total tokens in the batch (batch_size * target_seq_len)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset) # Average loss per sample\n",
        "    avg_acc = running_corrects / total_tokens # Accuracy across all tokens in the test set\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f} \\t Test Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    wandb.log({'Test_accuracy': avg_acc * 100})\n",
        "    wandb.log({ 'Test_loss': avg_loss})"
      ],
      "metadata": {
        "id": "Oyzmwoxv_48o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WANDB SWEEPS WITHOUT ATTENTION"
      ],
      "metadata": {
        "id": "KRsSChGHABHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep config file\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'sweep - no attention',\n",
        "    'metric': {\n",
        "      'goal': 'maximize',\n",
        "      'name': 'validation_accuracy'\n",
        "    },\n",
        "    'parameters':{\n",
        "        'input_embedding_size': {\n",
        "            'values': [64, 128] # 16,32,64,\n",
        "        },\n",
        "        'enc_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'dec_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values': [64, 128, 256]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['lstm','rnn','gru']\n",
        "        #},\n",
        "        #'bidirectional' : {\n",
        "        #   'values' : [True]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.1, 0.2, 0.3]\n",
        "        },\n",
        "        'beam_size' : {\n",
        "            'values' : [1,3,5]\n",
        "        }\n",
        "     }\n",
        "}\n",
        "\n",
        "# Create a sweep\n",
        "sweep_id = wandb.sweep(sweep = sweep_config, entity=\"mdkarimullahaque-iit-madras\", project='DL_Assignment_3') # modeling MA23C021-A3\n",
        "# f5557fe014798eefe63f9822700bf01578424638"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_5AWcVx8_-v4",
        "outputId": "986838a9-712c-4408-8507-8ae4f77587ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: qs9elj80\n",
            "Sweep URL: https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/qs9elj80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb sweeps without attention - make sure to use your key for running\n",
        "def main():\n",
        "  with wandb.init() as run:\n",
        "    wandb.run.name = f'cell-{wandb.config.cell_type}_hid_sz-{wandb.config.hidden_size}_inp_embed-{wandb.config.input_embedding_size}_enc-{wandb.config.enc_layers}_dec-{wandb.config.dec_layers}_dropout-{wandb.config.dropout}'\n",
        "\n",
        "    # Get the vocabulary sizes from the global variables\n",
        "    input_vocab_size = len(eng_chars_idx)\n",
        "    output_vocab_size = len(mal_chars_idx)\n",
        "\n",
        "    # Encoder part - Correct parameter names\n",
        "    encoder = Encoder(\n",
        "                    input_dim = input_vocab_size, # Use the actual input vocabulary size\n",
        "                    emb_dim = wandb.config.input_embedding_size, # Map sweep param to emb_dim\n",
        "                    enc_hid_dim =  wandb.config.hidden_size, # Map sweep param to enc_hid_dim\n",
        "                    cell_type = wandb.config.cell_type,\n",
        "                    num_layers = wandb.config.enc_layers, # Map sweep param to num_layers\n",
        "                    #bidirectional = wandb.config.bidirectional, # Uncomment if bidirectional is added\n",
        "                    dropout = wandb.config.dropout\n",
        "                    )\n",
        "    # Decoder part - Correct parameter names\n",
        "    decoder = Decoder(\n",
        "                        output_dim = output_vocab_size, # Use the actual output vocabulary size\n",
        "                        emb_dim = wandb.config.input_embedding_size, # Map sweep param to emb_dim\n",
        "                        dec_hid_dim = wandb.config.hidden_size, # Map sweep param to dec_hid_dim\n",
        "                        cell_type = wandb.config.cell_type,\n",
        "                        num_layers = wandb.config.dec_layers, # Map sweep param to num_layers\n",
        "                        dropout = wandb.config.dropout,\n",
        "                        #bidirectional = wandb.config.bidirectional, # Uncomment if bidirectional is added\n",
        "                        attention = False,\n",
        "                        attention_dim = wandb.config.hidden_size # This param might only be needed if attention is True\n",
        "                        )\n",
        "    # Init model - Correct parameter names if they exist in Seq2Seq __init__\n",
        "    # Note: Seq2Seq __init__ now takes encoder and decoder objects directly.\n",
        "    # The following parameters are passed to Seq2Seq: enc_hid_dim, dec_hid_dim,\n",
        "    # enc_num_layers, dec_num_layers, cell_type, dropout, attention.\n",
        "    model1 = Seq2Seq(encoder = encoder, # Pass the instantiated encoder object\n",
        "                     decoder = decoder, # Pass the instantiated decoder object\n",
        "                    enc_hid_dim = wandb.config.hidden_size, # Map sweep param\n",
        "                    dec_hid_dim = wandb.config.hidden_size, # Map sweep param\n",
        "                    #bidirectional = wandb.config.bidirectional, # Uncomment if bidirectional is added\n",
        "                    enc_num_layers = wandb.config.enc_layers, # Map sweep param\n",
        "                    dec_num_layers = wandb.config.dec_layers, # Map sweep param\n",
        "                    cell_type = wandb.config.cell_type, # Map sweep param\n",
        "                    dropout = wandb.config.dropout, # Map sweep param\n",
        "                    attention = False\n",
        "                    # beam_width is not a parameter of Seq2Seq __init__ based on your definition\n",
        "                    # device is also not a parameter of Seq2Seq __init__\n",
        "                )\n",
        "\n",
        "    model1.to(device)\n",
        "\n",
        "    epochs = 15\n",
        "    # train1 function signature is train1(model, train_loader, val_loader, epochs)\n",
        "    # The extra 'beam' argument seems incorrect based on the train1 definition\n",
        "    train1(model1, train_loader, val_loader, epochs) # Removed extra argument\n",
        "\n",
        "wandb.agent(sweep_id, function = main, count = 50) # calls main function for count number of times\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7a3BWnoHAF5p",
        "outputId": "212e1255-6406-4d52-9735-52e921b610ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fhul7g4i with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdkarimullahaque\u001b[0m (\u001b[33mmdkarimullahaque-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_075923-fhul7g4i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/fhul7g4i' target=\"_blank\">young-sweep-1</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/fhul7g4i' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/fhul7g4i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Train loss: 1.2506 \t Train Accuracy: 0.7252\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [01:27<20:30, 87.92s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Validation loss: 1.2046 \t Validation Accuracy: 0.7184\n",
            "Epoch no: 1\n",
            "Train loss: 0.7740 \t Train Accuracy: 0.7939\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [02:46<17:54, 82.66s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 1\n",
            "Validation loss: 1.1830 \t Validation Accuracy: 0.7330\n",
            "Epoch no: 2\n",
            "Train loss: 0.6895 \t Train Accuracy: 0.8166\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [04:06<16:13, 81.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 2\n",
            "Validation loss: 1.2786 \t Validation Accuracy: 0.7139\n",
            "Epoch no: 3\n",
            "Train loss: 0.6277 \t Train Accuracy: 0.8327\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [05:27<14:51, 81.08s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 3\n",
            "Validation loss: 1.2470 \t Validation Accuracy: 0.7324\n",
            "Epoch no: 4\n",
            "Train loss: 0.5706 \t Train Accuracy: 0.8468\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [06:46<13:23, 80.31s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 4\n",
            "Validation loss: 1.2161 \t Validation Accuracy: 0.7437\n",
            "Epoch no: 5\n",
            "Train loss: 0.5132 \t Train Accuracy: 0.8606\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [08:05<11:59, 79.93s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 5\n",
            "Validation loss: 1.1670 \t Validation Accuracy: 0.7585\n",
            "Epoch no: 6\n",
            "Train loss: 0.4584 \t Train Accuracy: 0.8736\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [09:24<10:36, 79.62s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 6\n",
            "Validation loss: 1.0964 \t Validation Accuracy: 0.7815\n",
            "Epoch no: 7\n",
            "Train loss: 0.4123 \t Train Accuracy: 0.8850\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [10:51<09:34, 82.08s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 7\n",
            "Validation loss: 1.0510 \t Validation Accuracy: 0.7938\n",
            "Epoch no: 8\n",
            "Train loss: 0.7702 \t Train Accuracy: 0.7924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [12:10<08:06, 81.05s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 8\n",
            "Validation loss: 0.6341 \t Validation Accuracy: 0.8210\n",
            "Epoch no: 9\n",
            "Train loss: 0.6753 \t Train Accuracy: 0.8060\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [13:29<06:42, 80.54s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 9\n",
            "Validation loss: 0.5924 \t Validation Accuracy: 0.8299\n",
            "Epoch no: 10\n",
            "Train loss: 0.6389 \t Train Accuracy: 0.8139\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [14:49<05:20, 80.24s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 10\n",
            "Validation loss: 0.5698 \t Validation Accuracy: 0.8340\n",
            "Epoch no: 11\n",
            "Train loss: 0.6123 \t Train Accuracy: 0.8194\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [16:07<03:58, 79.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 11\n",
            "Validation loss: 0.5454 \t Validation Accuracy: 0.8389\n",
            "Epoch no: 12\n",
            "Train loss: 0.5910 \t Train Accuracy: 0.8241\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [17:27<02:39, 79.62s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 12\n",
            "Validation loss: 0.5239 \t Validation Accuracy: 0.8448\n",
            "Epoch no: 13\n",
            "Train loss: 0.5723 \t Train Accuracy: 0.8287\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [18:47<01:19, 79.74s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 13\n",
            "Validation loss: 0.5101 \t Validation Accuracy: 0.8468\n",
            "Epoch no: 14\n",
            "Train loss: 0.5567 \t Train Accuracy: 0.8322\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [20:05<00:00, 80.37s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 14\n",
            "Validation loss: 0.4978 \t Validation Accuracy: 0.8512\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▇▇█▄▅▅▅▅▆▆</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▁▁▄▃▃▃▂▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▂▁▂▃▃▄▅▆▇▇▇███</td></tr><tr><td>validation_loss</td><td>▇▇██▇▇▆▆▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>train_accuracy</td><td>83.22433</td></tr><tr><td>train_loss</td><td>0.55674</td></tr><tr><td>validation_accuracy</td><td>85.11601</td></tr><tr><td>validation_loss</td><td>0.49781</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-lstm_hid_sz-64_inp_embed-128_enc-1_dec-1_dropout-0.2</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/fhul7g4i' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/fhul7g4i</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_075923-fhul7g4i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: szf6fgu6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_081937-szf6fgu6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/szf6fgu6' target=\"_blank\">volcanic-sweep-2</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/szf6fgu6' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/szf6fgu6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Train loss: 0.9561 \t Train Accuracy: 0.7487\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [05:27<1:16:18, 327.05s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Validation loss: 1.2153 \t Validation Accuracy: 0.7482\n",
            "Epoch no: 1\n",
            "Train loss: 0.6758 \t Train Accuracy: 0.8037\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [10:46<1:09:54, 322.63s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 1\n",
            "Validation loss: 1.1494 \t Validation Accuracy: 0.7326\n",
            "Epoch no: 2\n",
            "Train loss: 0.5569 \t Train Accuracy: 0.8339\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [16:05<1:04:09, 320.81s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 2\n",
            "Validation loss: 1.0773 \t Validation Accuracy: 0.7549\n",
            "Epoch no: 3\n",
            "Train loss: 0.4609 \t Train Accuracy: 0.8586\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [21:22<58:34, 319.48s/it]  "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 3\n",
            "Validation loss: 0.9939 \t Validation Accuracy: 0.7949\n",
            "Epoch no: 4\n",
            "Train loss: 0.3849 \t Train Accuracy: 0.8795\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [26:43<53:21, 320.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 4\n",
            "Validation loss: 0.9899 \t Validation Accuracy: 0.7986\n",
            "Epoch no: 5\n",
            "Train loss: 0.3287 \t Train Accuracy: 0.8956\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [32:08<48:14, 321.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 5\n",
            "Validation loss: 0.9042 \t Validation Accuracy: 0.8144\n",
            "Epoch no: 6\n",
            "Train loss: 0.2863 \t Train Accuracy: 0.9084\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [37:34<43:03, 322.91s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 6\n",
            "Validation loss: 0.8744 \t Validation Accuracy: 0.8197\n",
            "Epoch no: 7\n",
            "Train loss: 0.2530 \t Train Accuracy: 0.9187\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [42:58<37:44, 323.50s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 7\n",
            "Validation loss: 0.8108 \t Validation Accuracy: 0.8545\n",
            "Epoch no: 8\n",
            "Train loss: 0.5670 \t Train Accuracy: 0.8232\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [48:21<32:18, 323.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 8\n",
            "Validation loss: 0.4686 \t Validation Accuracy: 0.8506\n",
            "Epoch no: 9\n",
            "Train loss: 0.5394 \t Train Accuracy: 0.8298\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [53:41<26:51, 322.36s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 9\n",
            "Validation loss: 0.5525 \t Validation Accuracy: 0.8248\n",
            "Epoch no: 10\n",
            "Train loss: 0.4357 \t Train Accuracy: 0.8573\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [59:02<21:27, 321.90s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 10\n",
            "Validation loss: 0.4122 \t Validation Accuracy: 0.8571\n",
            "Epoch no: 11\n",
            "Train loss: 0.3858 \t Train Accuracy: 0.8743\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [1:04:22<16:03, 321.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 11\n",
            "Validation loss: 0.3657 \t Validation Accuracy: 0.8885\n",
            "Epoch no: 12\n",
            "Train loss: 0.3591 \t Train Accuracy: 0.8835\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [1:09:43<10:42, 321.15s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 12\n",
            "Validation loss: 0.3517 \t Validation Accuracy: 0.8932\n",
            "Epoch no: 13\n",
            "Train loss: 0.3367 \t Train Accuracy: 0.8917\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [1:15:00<05:20, 320.04s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 13\n",
            "Validation loss: 0.3777 \t Validation Accuracy: 0.8775\n",
            "Epoch no: 14\n",
            "Train loss: 0.4020 \t Train Accuracy: 0.8665\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [1:20:16<00:00, 321.11s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 14\n",
            "Validation loss: 0.3770 \t Validation Accuracy: 0.8743\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▆▇██▄▄▅▆▇▇▆</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▄▄▃▂▂▂▂</td></tr><tr><td>validation_accuracy</td><td>▂▁▂▄▄▅▅▆▆▅▆██▇▇</td></tr><tr><td>validation_loss</td><td>█▇▇▆▆▅▅▅▂▃▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>train_accuracy</td><td>86.65487</td></tr><tr><td>train_loss</td><td>0.40196</td></tr><tr><td>validation_accuracy</td><td>87.42701</td></tr><tr><td>validation_loss</td><td>0.377</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-gru_hid_sz-256_inp_embed-64_enc-2_dec-1_dropout-0.2</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/szf6fgu6' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/szf6fgu6</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_081937-szf6fgu6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j29i91xo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 128\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_094000-j29i91xo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/j29i91xo' target=\"_blank\">amber-sweep-3</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/j29i91xo' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/j29i91xo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-16-5b44e9432a02>\", line 55, in main\n",
            "    train1(model1, train_loader, val_loader, epochs) # Removed extra argument\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-1a4a35a7af27>\", line 37, in train1\n",
            "    outputs, _ = model(inputs, labels, epoch < epochs/2, phase == 'train')\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-13-2d0e953aaefd>\", line 137, in forward\n",
            "    decoder_output, decoder_hidden, cell_state, attention_wts = self.decoder(\n",
            "                                                                ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-12-9cd3e04cba93>\", line 169, in forward\n",
            "    rnn_output, (hidden, cell) = self.rnn(output, (hidden, cell)) # Pass tuple for LSTM\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1120, in forward\n",
            "    self.check_forward_args(input, hx, batch_sizes)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1008, in check_forward_args\n",
            "    self.check_hidden_size(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 347, in check_hidden_size\n",
            "    raise RuntimeError(msg.format(expected_hidden_size, list(hx.size())))\n",
            "RuntimeError: Expected hidden[1] size (1, 256, 64), got [3, 256, 64]\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-lstm_hid_sz-64_inp_embed-128_enc-3_dec-1_dropout-0.1</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/j29i91xo' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/j29i91xo</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_094000-j29i91xo/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run j29i91xo errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-16-5b44e9432a02>\", line 55, in main\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train1(model1, train_loader, val_loader, epochs) # Removed extra argument\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-14-1a4a35a7af27>\", line 37, in train1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs, _ = model(inputs, labels, epoch < epochs/2, phase == 'train')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-13-2d0e953aaefd>\", line 137, in forward\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     decoder_output, decoder_hidden, cell_state, attention_wts = self.decoder(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                                                 ^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-12-9cd3e04cba93>\", line 169, in forward\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     rnn_output, (hidden, cell) = self.rnn(output, (hidden, cell)) # Pass tuple for LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1120, in forward\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.check_forward_args(input, hx, batch_sizes)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1008, in check_forward_args\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.check_hidden_size(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 347, in check_hidden_size\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise RuntimeError(msg.format(expected_hidden_size, list(hx.size())))\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Expected hidden[1] size (1, 256, 64), got [3, 256, 64]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n7tquqp6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_094005-n7tquqp6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/n7tquqp6' target=\"_blank\">misunderstood-sweep-4</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/n7tquqp6' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/n7tquqp6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Train loss: 0.9718 \t Train Accuracy: 0.7479\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [09:34<2:14:06, 574.78s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Validation loss: 1.2607 \t Validation Accuracy: 0.7016\n",
            "Epoch no: 1\n",
            "Train loss: 0.6666 \t Train Accuracy: 0.8059\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [19:09<2:04:33, 574.90s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 1\n",
            "Validation loss: 1.1466 \t Validation Accuracy: 0.7405\n",
            "Epoch no: 2\n",
            "Train loss: 0.4856 \t Train Accuracy: 0.8532\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [28:48<1:55:17, 576.43s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 2\n",
            "Validation loss: 1.0152 \t Validation Accuracy: 0.8005\n",
            "Epoch no: 3\n",
            "Train loss: 0.3642 \t Train Accuracy: 0.8863\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [38:17<1:45:08, 573.52s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 3\n",
            "Validation loss: 0.8928 \t Validation Accuracy: 0.8317\n",
            "Epoch no: 4\n",
            "Train loss: 0.2797 \t Train Accuracy: 0.9113\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [47:41<1:35:00, 570.09s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 4\n",
            "Validation loss: 0.8426 \t Validation Accuracy: 0.8250\n",
            "Epoch no: 5\n",
            "Train loss: 0.2283 \t Train Accuracy: 0.9274\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [57:09<1:25:26, 569.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 5\n",
            "Validation loss: 0.7764 \t Validation Accuracy: 0.8419\n",
            "Epoch no: 6\n",
            "Train loss: 0.1918 \t Train Accuracy: 0.9390\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [1:06:40<1:15:59, 569.89s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 6\n",
            "Validation loss: 0.6635 \t Validation Accuracy: 0.8874\n",
            "Epoch no: 7\n",
            "Train loss: 0.1638 \t Train Accuracy: 0.9487\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [1:16:10<1:06:29, 569.98s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 7\n",
            "Validation loss: 0.6833 \t Validation Accuracy: 0.8875\n",
            "Epoch no: 8\n",
            "Train loss: 0.5336 \t Train Accuracy: 0.8339\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [1:25:44<57:07, 571.32s/it]  "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 8\n",
            "Validation loss: 0.4339 \t Validation Accuracy: 0.8529\n",
            "Epoch no: 9\n",
            "Train loss: 0.3757 \t Train Accuracy: 0.8776\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [1:35:15<47:35, 571.07s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 9\n",
            "Validation loss: 0.3310 \t Validation Accuracy: 0.8976\n",
            "Epoch no: 10\n",
            "Train loss: 0.3601 \t Train Accuracy: 0.8829\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [1:44:47<38:06, 571.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 10\n",
            "Validation loss: 0.4654 \t Validation Accuracy: 0.8663\n",
            "Epoch no: 11\n",
            "Train loss: 0.3211 \t Train Accuracy: 0.8942\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [1:54:12<28:28, 569.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 11\n",
            "Validation loss: 0.3035 \t Validation Accuracy: 0.9130\n",
            "Epoch no: 12\n",
            "Train loss: 0.2745 \t Train Accuracy: 0.9076\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [2:03:35<18:55, 567.67s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 12\n",
            "Validation loss: 0.3517 \t Validation Accuracy: 0.8959\n",
            "Epoch no: 13\n",
            "Train loss: 0.2702 \t Train Accuracy: 0.9147\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [2:12:55<09:25, 565.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 13\n",
            "Validation loss: 0.2845 \t Validation Accuracy: 0.9147\n",
            "Epoch no: 14\n",
            "Train loss: 0.2280 \t Train Accuracy: 0.9282\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [2:22:17<00:00, 569.18s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 14\n",
            "Validation loss: 0.2743 \t Validation Accuracy: 0.9191\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▇▇██▄▆▆▆▇▇▇</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▄▃▃▂▂▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▂▄▅▅▆▇▇▆▇▆█▇██</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▅▄▄▂▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>train_accuracy</td><td>92.81721</td></tr><tr><td>train_loss</td><td>0.22805</td></tr><tr><td>validation_accuracy</td><td>91.90881</td></tr><tr><td>validation_loss</td><td>0.27426</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-gru_hid_sz-256_inp_embed-64_enc-3_dec-2_dropout-0.2</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/n7tquqp6' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/n7tquqp6</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_094005-n7tquqp6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7fm42vm1 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_120228-7fm42vm1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/7fm42vm1' target=\"_blank\">likely-sweep-5</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/7fm42vm1' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/7fm42vm1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Train loss: 0.9977 \t Train Accuracy: 0.7396\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [11:31<2:41:17, 691.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Validation loss: 1.1914 \t Validation Accuracy: 0.7450\n",
            "Epoch no: 1\n",
            "Train loss: 0.6985 \t Train Accuracy: 0.7982\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [22:59<2:29:23, 689.49s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 1\n",
            "Validation loss: 1.1493 \t Validation Accuracy: 0.7401\n",
            "Epoch no: 2\n",
            "Train loss: 0.5109 \t Train Accuracy: 0.8459\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [34:37<2:18:42, 693.54s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 2\n",
            "Validation loss: 1.0100 \t Validation Accuracy: 0.7953\n",
            "Epoch no: 3\n",
            "Train loss: 0.3788 \t Train Accuracy: 0.8814\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [46:09<2:07:02, 692.98s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 3\n",
            "Validation loss: 0.8550 \t Validation Accuracy: 0.8405\n",
            "Epoch no: 4\n",
            "Train loss: 0.2903 \t Train Accuracy: 0.9077\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [57:48<1:55:51, 695.14s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 4\n",
            "Validation loss: 0.8260 \t Validation Accuracy: 0.8345\n",
            "Epoch no: 5\n",
            "Train loss: 0.2306 \t Train Accuracy: 0.9265\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [1:09:41<1:45:09, 701.05s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 5\n",
            "Validation loss: 0.7494 \t Validation Accuracy: 0.8501\n",
            "Epoch no: 6\n",
            "Train loss: 0.1925 \t Train Accuracy: 0.9392\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [1:21:25<1:33:37, 702.17s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 6\n",
            "Validation loss: 0.6484 \t Validation Accuracy: 0.8952\n",
            "Epoch no: 7\n",
            "Train loss: 0.1618 \t Train Accuracy: 0.9495\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [1:33:16<1:22:14, 704.95s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 7\n",
            "Validation loss: 0.6154 \t Validation Accuracy: 0.9031\n",
            "Epoch no: 8\n",
            "Train loss: 0.4432 \t Train Accuracy: 0.8617\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [1:44:54<1:10:15, 702.61s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 8\n",
            "Validation loss: 0.3332 \t Validation Accuracy: 0.8994\n",
            "Epoch no: 9\n",
            "Train loss: 0.3504 \t Train Accuracy: 0.8890\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [1:56:38<58:36, 703.23s/it] "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 9\n",
            "Validation loss: 0.3293 \t Validation Accuracy: 0.9012\n",
            "Epoch no: 10\n",
            "Train loss: 0.3011 \t Train Accuracy: 0.9051\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [2:08:28<47:01, 705.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 10\n",
            "Validation loss: 0.3044 \t Validation Accuracy: 0.9082\n",
            "Epoch no: 11\n",
            "Train loss: 0.2725 \t Train Accuracy: 0.9146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [2:20:03<35:05, 701.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 11\n",
            "Validation loss: 0.2946 \t Validation Accuracy: 0.9137\n",
            "Epoch no: 12\n",
            "Train loss: 0.2325 \t Train Accuracy: 0.9271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [2:31:44<23:23, 701.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 12\n",
            "Validation loss: 0.3216 \t Validation Accuracy: 0.9034\n",
            "Epoch no: 13\n",
            "Train loss: 0.2142 \t Train Accuracy: 0.9332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [2:43:20<11:39, 699.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 13\n",
            "Validation loss: 0.2757 \t Validation Accuracy: 0.9210\n",
            "Epoch no: 14\n",
            "Train loss: 0.1944 \t Train Accuracy: 0.9395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [2:54:54<00:00, 699.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 14\n",
            "Validation loss: 0.2542 \t Validation Accuracy: 0.9278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▇▇██▅▆▇▇▇▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▃▃▂▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▃▅▅▅▇▇▇▇▇▇▇██</td></tr><tr><td>validation_loss</td><td>██▇▅▅▅▄▄▂▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>14</td></tr><tr><td>train_accuracy</td><td>93.95398</td></tr><tr><td>train_loss</td><td>0.19439</td></tr><tr><td>validation_accuracy</td><td>92.7753</td></tr><tr><td>validation_loss</td><td>0.2542</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-gru_hid_sz-256_inp_embed-64_enc-3_dec-3_dropout-0.3</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/7fm42vm1' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/7fm42vm1</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250519_120228-7fm42vm1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5uqbcmb8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_145734-5uqbcmb8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/5uqbcmb8' target=\"_blank\">woven-sweep-6</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/sfyyeqvb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/5uqbcmb8' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/5uqbcmb8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 0\n",
            "Train loss: 1.0267 \t Train Accuracy: 0.7328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [11:31<2:41:24, 691.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 0\n",
            "Validation loss: 1.2134 \t Validation Accuracy: 0.7205\n",
            "Epoch no: 1\n",
            "Train loss: 0.6794 \t Train Accuracy: 0.8064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [22:59<2:29:20, 689.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 1\n",
            "Validation loss: 1.1350 \t Validation Accuracy: 0.7498\n",
            "Epoch no: 2\n",
            "Train loss: 0.4690 \t Train Accuracy: 0.8583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [34:24<2:17:31, 687.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 2\n",
            "Validation loss: 0.9208 \t Validation Accuracy: 0.8238\n",
            "Epoch no: 3\n",
            "Train loss: 0.3317 \t Train Accuracy: 0.8959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [45:53<2:06:06, 687.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 3\n",
            "Validation loss: 0.8025 \t Validation Accuracy: 0.8555\n",
            "Epoch no: 4\n",
            "Train loss: 0.2585 \t Train Accuracy: 0.9188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [57:26<1:54:58, 689.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 4\n",
            "Validation loss: 0.6857 \t Validation Accuracy: 0.8814\n",
            "Epoch no: 5\n",
            "Train loss: 0.2007 \t Train Accuracy: 0.9363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [1:09:02<1:43:47, 691.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 5\n",
            "Validation loss: 0.6727 \t Validation Accuracy: 0.8774\n",
            "Epoch no: 6\n",
            "Train loss: 0.1784 \t Train Accuracy: 0.9436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [1:20:39<1:32:29, 693.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 6\n",
            "Validation loss: 0.6150 \t Validation Accuracy: 0.8894\n",
            "Epoch no: 7\n",
            "Train loss: 0.1651 \t Train Accuracy: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [1:32:23<1:21:18, 696.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 7\n",
            "Validation loss: 0.5981 \t Validation Accuracy: 0.9014\n",
            "Epoch no: 8\n",
            "Train loss: 0.4408 \t Train Accuracy: 0.8634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [1:43:53<1:09:28, 694.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 8\n",
            "Validation loss: 0.3256 \t Validation Accuracy: 0.9033\n",
            "Epoch no: 9\n",
            "Train loss: 0.3305 \t Train Accuracy: 0.8936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [1:55:32<57:59, 695.93s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 9\n",
            "Validation loss: 0.3047 \t Validation Accuracy: 0.9119\n",
            "Epoch no: 10\n",
            "Train loss: 0.2848 \t Train Accuracy: 0.9100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [2:07:11<46:27, 696.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 10\n",
            "Validation loss: 0.2889 \t Validation Accuracy: 0.9137\n",
            "Epoch no: 11\n",
            "Train loss: 0.2648 \t Train Accuracy: 0.9171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [2:18:53<34:55, 698.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 11\n",
            "Validation loss: 0.2717 \t Validation Accuracy: 0.9190\n",
            "Epoch no: 12\n",
            "Train loss: 0.2340 \t Train Accuracy: 0.9273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [2:30:28<23:14, 697.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 12\n",
            "Validation loss: 0.2923 \t Validation Accuracy: 0.9100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Accuracy calculation"
      ],
      "metadata": {
        "id": "jNO0ye1mAW0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calc_test_acc(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_score = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs, _ = model(inputs, None, False, False)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(F.softmax(outputs, dim=2), dim=2).T\n",
        "        total_score += compute_score(preds, targets)\n",
        "\n",
        "        # Reshape outputs and targets for loss calculation\n",
        "        outputs = outputs.permute(1, 0, 2).reshape(-1, 72)\n",
        "        targets = F.one_hot(targets, num_classes=72).float().reshape(-1, 72)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    avg_score = total_score / len(loader.dataset)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss} \\t Test Accuracy: {avg_score}')\n",
        "\n",
        "    wandb.log({'Test_accuracy': avg_score * 100})\n",
        "    wandb.log({ 'Test_loss': avg_loss})\n",
        "\n"
      ],
      "metadata": {
        "id": "spqABOcBAX6D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best hyperparameter configuration without attention\n",
        "'''\n",
        "input embedding size: 256\n",
        "number of encoder layers: 3\n",
        "number of decoder layers: 2 -> This should be 3 based on the error and the code below\n",
        "hidden layer size: 256\n",
        "cell type: LSTM\n",
        "#bidirectional: True\n",
        "dropout: 0.2 -> Using 0.3 based on the code below\n",
        "beam width : 1\n",
        "'''\n",
        "\n",
        "# Instantiate the Encoder and Decoder first\n",
        "# Use len(eng_chars_idx) and len(mal_chars_idx) for input_dim and output_dim respectively\n",
        "# These global variables are available from previous code blocks.\n",
        "encoder_best = Encoder(\n",
        "    input_dim=len(eng_chars_idx), # Use the size of the English character index as input_dim\n",
        "    emb_dim=256,\n",
        "    enc_hid_dim=256,\n",
        "    cell_type='lstm',\n",
        "    num_layers=3,\n",
        "    dropout=0.3 # Using 0.3 as in the original code block\n",
        "    # Remove bidirectional if not used\n",
        ")\n",
        "\n",
        "decoder_best = Decoder(\n",
        "    output_dim=len(mal_chars_idx), # Use the size of the Malayalam character index as output_dim\n",
        "    emb_dim=256,\n",
        "    dec_hid_dim=256,\n",
        "    cell_type='lstm',\n",
        "    num_layers=3, # Changed from 2 to 3 to match the Seq2Seq instantiation below\n",
        "    dropout=0.3, # Using 0.3 as in the original code block\n",
        "    # Remove bidirectional if not used\n",
        "    attention=False # No attention for this model\n",
        ")\n",
        "\n",
        "# Instantiate Seq2Seq with the encoder and decoder objects and correct parameter names\n",
        "# The Seq2Seq __init__ takes encoder and decoder objects directly, plus some config parameters.\n",
        "best_model = Seq2Seq(\n",
        "    encoder=encoder_best, # Pass the instantiated encoder object\n",
        "    decoder=decoder_best, # Pass the instantiated decoder object\n",
        "    enc_hid_dim=256, # Pass encoder hidden dim\n",
        "    dec_hid_dim=256, # Pass decoder hidden dim\n",
        "    #bidirectional = True, # Uncomment if bidirectional is used\n",
        "    enc_num_layers=3, # Pass encoder num layers\n",
        "    dec_num_layers=3, # Changed from 2 to 3 to match decoder_best.num_layers\n",
        "    cell_type='lstm', # Pass cell type\n",
        "    dropout=0.3, # Pass dropout\n",
        "    attention=False # Pass attention flag\n",
        "    # Remove beam_width and device as they are not arguments for Seq2Seq __init__\n",
        ")\n",
        "\n",
        "best_model.to(device)\n",
        "epochs = 20\n",
        "\n",
        "# Wrap the training call for the best_model in a wandb.init() block\n",
        "# Give it a name so it appears separately in your dashboard\n",
        "with wandb.init(project='DL_Assignment_3', entity=\"mdkarimullahaque-iit-madras\", name='Best_Model_Training') as run:\n",
        "    # Ensure the train1 function call matches its definition: train1(model, train_loader, val_loader, epochs)\n",
        "    train1(best_model, train_loader, val_loader, epochs) # Removed the extra 'False' argument\n",
        "\n",
        "# If you also want to calculate and log test accuracy after training the best model\n",
        "# Add the call to calc_test_acc within the same wandb.init block or another one\n",
        "# using the trained best_model\n",
        "with wandb.init(project='DL_Assignment_3', entity=\"mdkarimullahaque-iit-madras\", name='Best_Model_Test_Accuracy') as run:\n",
        "    calc_test_acc(best_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7TBkMH-Pxtv5",
        "outputId": "075123b5-eae6-4e14-8f0e-452ef69c6637"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdkarimullahaque\u001b[0m (\u001b[33mmdkarimullahaque-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_045352-yallob6m</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/yallob6m' target=\"_blank\">Best_Model_Training</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/yallob6m' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/yallob6m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Train loss: 1.0156 \t Train Accuracy: 0.7457\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 1/20 [17:51<5:39:22, 1071.74s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 0\n",
            "Validation loss: 1.1797 \t Validation Accuracy: 0.7429\n",
            "Epoch no: 1\n",
            "Train loss: 0.7104 \t Train Accuracy: 0.8067\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 2/20 [35:44<5:21:45, 1072.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 1\n",
            "Validation loss: 1.2888 \t Validation Accuracy: 0.7427\n",
            "Epoch no: 2\n",
            "Train loss: 0.5701 \t Train Accuracy: 0.8433\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 3/20 [53:52<5:05:48, 1079.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 2\n",
            "Validation loss: 1.2967 \t Validation Accuracy: 0.7544\n",
            "Epoch no: 3\n",
            "Train loss: 0.4085 \t Train Accuracy: 0.8856\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 4/20 [1:11:49<4:47:36, 1078.52s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 3\n",
            "Validation loss: 1.0384 \t Validation Accuracy: 0.8169\n",
            "Epoch no: 4\n",
            "Train loss: 0.2654 \t Train Accuracy: 0.9234\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 5/20 [1:29:56<4:30:22, 1081.49s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 4\n",
            "Validation loss: 0.8266 \t Validation Accuracy: 0.8618\n",
            "Epoch no: 5\n",
            "Train loss: 0.1877 \t Train Accuracy: 0.9452\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 6/20 [1:47:59<4:12:28, 1082.06s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 5\n",
            "Validation loss: 0.6712 \t Validation Accuracy: 0.8925\n",
            "Epoch no: 6\n",
            "Train loss: 0.1472 \t Train Accuracy: 0.9565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 7/20 [2:05:59<3:54:18, 1081.43s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 6\n",
            "Validation loss: 0.5929 \t Validation Accuracy: 0.9089\n",
            "Epoch no: 7\n",
            "Train loss: 0.1191 \t Train Accuracy: 0.9648\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 8/20 [2:24:07<3:36:42, 1083.54s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch no: 7\n",
            "Validation loss: 0.5673 \t Validation Accuracy: 0.9163\n",
            "Epoch no: 8\n",
            "Train loss: 0.1015 \t Train Accuracy: 0.9697\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 9/20 [2:42:11<3:18:38, 1083.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 8\n",
            "Validation loss: 0.5485 \t Validation Accuracy: 0.9212\n",
            "Epoch no: 9\n",
            "Train loss: 0.0883 \t Train Accuracy: 0.9736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [3:00:16<3:00:41, 1084.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 9\n",
            "Validation loss: 0.5336 \t Validation Accuracy: 0.9250\n",
            "Epoch no: 10\n",
            "Train loss: 0.3150 \t Train Accuracy: 0.9101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [3:18:15<2:42:22, 1082.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 10\n",
            "Validation loss: 0.2642 \t Validation Accuracy: 0.9254\n",
            "Epoch no: 11\n",
            "Train loss: 0.2312 \t Train Accuracy: 0.9318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [3:36:18<2:24:21, 1082.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 11\n",
            "Validation loss: 0.2512 \t Validation Accuracy: 0.9287\n",
            "Epoch no: 12\n",
            "Train loss: 0.1990 \t Train Accuracy: 0.9410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [3:54:19<2:06:14, 1082.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 12\n",
            "Validation loss: 0.2391 \t Validation Accuracy: 0.9320\n",
            "Epoch no: 13\n",
            "Train loss: 0.1775 \t Train Accuracy: 0.9474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [4:12:19<1:48:09, 1081.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 13\n",
            "Validation loss: 0.2356 \t Validation Accuracy: 0.9352\n",
            "Epoch no: 14\n",
            "Train loss: 0.1573 \t Train Accuracy: 0.9534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [4:30:25<1:30:14, 1082.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 14\n",
            "Validation loss: 0.2291 \t Validation Accuracy: 0.9379\n",
            "Epoch no: 15\n",
            "Train loss: 0.1427 \t Train Accuracy: 0.9576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [4:48:29<1:12:13, 1083.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 15\n",
            "Validation loss: 0.2362 \t Validation Accuracy: 0.9356\n",
            "Epoch no: 16\n",
            "Train loss: 0.1331 \t Train Accuracy: 0.9602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [5:06:42<54:18, 1086.27s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 16\n",
            "Validation loss: 0.2363 \t Validation Accuracy: 0.9366\n",
            "Epoch no: 17\n",
            "Train loss: 0.1236 \t Train Accuracy: 0.9631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [5:24:50<36:13, 1086.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch no: 17\n",
            "Validation loss: 0.2368 \t Validation Accuracy: 0.9381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep config file\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'testset run',\n",
        "    'metric': {\n",
        "      'goal': 'maximize',\n",
        "      'name': 'test_accuracy'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'beam_size':{\n",
        "            'values': [1]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "aMR2Ul9cAeSl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sweep\n",
        "sweep_id = wandb.sweep(sweep = sweep_config, entity=\"mdkarimullahaque-iit-madras\", project='DL_Assignment_3')"
      ],
      "metadata": {
        "id": "j3INT0FfAgYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84d2f0d-7e5d-4dba-cd83-57ff4d64898b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: iehkqnj9\n",
            "Sweep URL: https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/iehkqnj9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb log for test accuracy\n",
        "def main():\n",
        "  with wandb.init() as run:\n",
        "    #run_name = \"-f_num_\"+str(wandb.config.filters_num)+\"-f_num_\"+wandb.config.filter_org+\"-ac_fn_\"+wandb.config.act_fn+\\\n",
        "                #\"-b_norm_\"+str(wandb.config.batch_norm) + \"-bs_\"+str(wandb.config.batch_size) +\"-neu_num\"+str(wandb.config.num_neurons_dense)\n",
        "\n",
        "    wandb.run.name = \"test_set_run\"\n",
        "    calc_test_acc(best_model, test_loader)\n",
        "\n",
        "wandb.agent(sweep_id, function = main, count = 1)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "h2KwiHL1AiTW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "b5d7c4b4-3d1a-4970-c4b9-1191b297bd4f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2pkytu97 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_113734-2pkytu97</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/2pkytu97' target=\"_blank\">playful-sweep-4</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/2pkytu97' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/2pkytu97</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-27-d8d18fff0d25>\", line 8, in main\n",
            "    calc_test_acc(best_model, test_loader)\n",
            "                  ^^^^^^^^^^\n",
            "NameError: name 'best_model' is not defined\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">test_set_run</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/2pkytu97' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/2pkytu97</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_113734-2pkytu97/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2pkytu97 errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-27-d8d18fff0d25>\", line 8, in main\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     calc_test_acc(best_model, test_loader)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'best_model' is not defined\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wandb sweeps with attention"
      ],
      "metadata": {
        "id": "jVHd_rrbAkyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep config file\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'sweep - attention',\n",
        "    'metric': {\n",
        "      'goal': 'maximize',\n",
        "      'name': 'validation_accuracy'\n",
        "    },\n",
        "    'parameters':{\n",
        "        'input_embedding_size': {\n",
        "            'values': [32, 64, 128] # 16,32,64,\n",
        "        },\n",
        "        'enc_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'dec_layers': {\n",
        "            'values': [1,2,3]\n",
        "\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values': [64, 128, 256]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['lstm','rnn','gru']\n",
        "        #},\n",
        "        #'bidirectional' : {\n",
        "           # 'values' : [True]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.1, 0.2, 0.3]\n",
        "        },\n",
        "        'beam_size' : {\n",
        "            'values' : [1,3,5]\n",
        "        }\n",
        "     }\n",
        "}\n",
        "\n",
        "# Create a sweep\n",
        "sweep_id = wandb.sweep(sweep = sweep_config, entity=\"mdkarimullahaque-iit-madras\", project='DL_Assignment_3')\n"
      ],
      "metadata": {
        "id": "qELCPNp5AloW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dcf6802-7ffd-4bec-840f-3164218eee6d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: z8h0iary\n",
            "Sweep URL: https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb sweeps with attention\n",
        "def main():\n",
        "  with wandb.init() as run:\n",
        "    wandb.run.name = f'cell-{wandb.config.cell_type}_hid_sz-{wandb.config.hidden_size}_inp_embed-{wandb.config.input_embedding_size}_enc-{wandb.config.enc_layers}_dec-{wandb.config.dec_layers}_dropout-{wandb.config.dropout}'\n",
        "\n",
        "    model1 = Seq2Seq(\n",
        "        encoder_hidden_dimension = wandb.config.hidden_size,\n",
        "        decoder_hidden_dimension = wandb.config.hidden_size,\n",
        "        encoder_embed_dimension =  wandb.config.input_embedding_size,\n",
        "        decoder_embed_dimension =  wandb.config.input_embedding_size,\n",
        "        #bidirectional = wandb.config.bidirectional,\n",
        "        encoder_num_layers = wandb.config.enc_layers,\n",
        "        decoder_num_layers = wandb.config.dec_layers,\n",
        "        cell_type = wandb.config.cell_type,\n",
        "        dropout = wandb.config.dropout,\n",
        "        beam_width = wandb.config.beam_size,\n",
        "        device = device,\n",
        "        attention = True\n",
        "    )\n",
        "\n",
        "\n",
        "    model1.to(device)\n",
        "    beam = False\n",
        "\n",
        "    epochs = 15\n",
        "    train1(model1, train_loader, val_loader, epochs, beam)\n",
        "\n",
        "wandb.agent(sweep_id, function = main, count = 25) # calls main function for count number of times\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6qBiZGznAnfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35769ac2-d9c8-474c-94bb-78a389b67f55"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s4sj5kpk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_113602-s4sj5kpk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/s4sj5kpk' target=\"_blank\">pretty-sweep-1</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/s4sj5kpk' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/s4sj5kpk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-25-ec00833857b3>\", line 6, in main\n",
            "    model1 = Seq2Seq(\n",
            "             ^^^^^^^^\n",
            "TypeError: Seq2Seq.__init__() got an unexpected keyword argument 'encoder_hidden_dimension'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-rnn_hid_sz-128_inp_embed-64_enc-1_dec-2_dropout-0.3</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/s4sj5kpk' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/s4sj5kpk</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_113602-s4sj5kpk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run s4sj5kpk errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-25-ec00833857b3>\", line 6, in main\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model1 = Seq2Seq(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m TypeError: Seq2Seq.__init__() got an unexpected keyword argument 'encoder_hidden_dimension'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4cr6waye with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_113607-4cr6waye</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/4cr6waye' target=\"_blank\">giddy-sweep-2</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/4cr6waye' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/4cr6waye</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-25-ec00833857b3>\", line 6, in main\n",
            "    model1 = Seq2Seq(\n",
            "             ^^^^^^^^\n",
            "TypeError: Seq2Seq.__init__() got an unexpected keyword argument 'encoder_hidden_dimension'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-rnn_hid_sz-128_inp_embed-32_enc-2_dec-1_dropout-0.1</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/4cr6waye' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/4cr6waye</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_113607-4cr6waye/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4cr6waye errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-25-ec00833857b3>\", line 6, in main\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model1 = Seq2Seq(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m TypeError: Seq2Seq.__init__() got an unexpected keyword argument 'encoder_hidden_dimension'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x5bb0sc7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embedding_size: 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_113612-x5bb0sc7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/x5bb0sc7' target=\"_blank\">driven-sweep-3</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/z8h0iary</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/x5bb0sc7' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/x5bb0sc7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-25-ec00833857b3>\", line 6, in main\n",
            "    model1 = Seq2Seq(\n",
            "             ^^^^^^^^\n",
            "TypeError: Seq2Seq.__init__() got an unexpected keyword argument 'encoder_hidden_dimension'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cell-rnn_hid_sz-256_inp_embed-64_enc-3_dec-3_dropout-0.2</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/x5bb0sc7' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/x5bb0sc7</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_113612-x5bb0sc7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run x5bb0sc7 errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-25-ec00833857b3>\", line 6, in main\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model1 = Seq2Seq(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m TypeError: Seq2Seq.__init__() got an unexpected keyword argument 'encoder_hidden_dimension'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder part\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim = 29,\n",
        "                 emb_dim = 256,\n",
        "                 dec_hid_dim = 256,\n",
        "                 cell_type='gru',\n",
        "                 num_layers=2, # Store this attribute\n",
        "                 dropout = 0,\n",
        "                 #bidirectional = True, # Keep commented if not using bidirectional\n",
        "                 attention = False,\n",
        "                 attention_dim = None,\n",
        "                 encoder_hid_dim=256, # Added for attention context size calculation\n",
        "                 encoder_val_direction=1 # Added for attention context size calculation\n",
        "                 ):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        device = torch.device('cuda' if torch.cuda.cuda() else 'cpu')\n",
        "        # Store num_layers as an attribute\n",
        "        self.num_layers = num_layers # <--- Add this line\n",
        "        # Embedding part\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.attention = attention\n",
        "        # Dropout to add onto embedded input\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.cell_type = cell_type\n",
        "        self.dec_hid_dim = dec_hid_dim # Store dec_hid_dim as well for potential use\n",
        "\n",
        "        # Define val_direction - assuming not using bidirectional decoder\n",
        "        self.val_direction = 1\n",
        "\n",
        "        # Linear layer to get the output\n",
        "        self.W1 = nn.Linear(dec_hid_dim * self.val_direction, output_dim)\n",
        "        # Softmax layer\n",
        "        self.softmax = F.softmax # This is not used in the forward pass when CrossEntropyLoss is used\n",
        "\n",
        "        # Determine the input size for the RNN/GRU based on whether attention is used for concatenation\n",
        "        rnn_input_size = emb_dim\n",
        "        if attention:\n",
        "             # If context is concatenated to the input embedding before the RNN/GRU\n",
        "             # Need to get the size of the context vector from the calculate_attention logic\n",
        "             # Based on Seq2Seq and Decoder forward, context comes from encoder_outputs (batch_size, seq_len, enc_hid_dim * val_direction)\n",
        "             # calculate_attention output is (batch_size, 1, enc_hid_dim * val_direction)\n",
        "             # So context size is enc_hid_dim * encoder_val_direction\n",
        "             rnn_input_size = emb_dim + encoder_hid_dim * encoder_val_direction # Use passed encoder dims\n",
        "\n",
        "\n",
        "        if cell_type.lower() == 'rnn':\n",
        "            self.rnn = nn.RNN(input_size=rnn_input_size, # Use rnn_input_size\n",
        "                              hidden_size = dec_hid_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout = dropout,\n",
        "                              #bidirectional= bidirectional, # Keep commented if not using bidirectional\n",
        "                              batch_first=True)\n",
        "        elif cell_type.lower() == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_size=rnn_input_size, # Use rnn_input_size\n",
        "                               hidden_size = dec_hid_dim,\n",
        "                               num_layers=num_layers,\n",
        "                               dropout = dropout,\n",
        "                               #bidirectional= bidirectional, # Keep commented if not using bidirectional\n",
        "                               batch_first=True)\n",
        "        elif cell_type.lower() == 'gru':\n",
        "            self.rnn = nn.GRU(input_size=rnn_input_size, # Use rnn_input_size\n",
        "                              hidden_size = dec_hid_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              dropout = dropout,\n",
        "                              #bidirectional= bidirectional, # Keep commented if not using bidirectional\n",
        "                              batch_first=True)\n",
        "\n",
        "        # self.fc_out = nn.Linear(dec_hid_dim, output_dim) # This layer is not used in the forward pass\n",
        "\n",
        "    # The calculate_attention function needs to be a method of the class if it uses self attributes, or a standalone function\n",
        "    # If it's a method, it needs 'self' as the first parameter.\n",
        "    # Based on its structure, it seems intended to be a standalone helper function or a method needing more context (like encoder_outputs structure)\n",
        "    # Let's move it outside the class definition for now, assuming it's a helper.\n",
        "    # Or, more likely, it should be integrated into the Decoder forward method directly if it's simple attention.\n",
        "    # If it's a complex attention module, it should be a separate nn.Module.\n",
        "    # Given it's defined *inside* the Decoder class but without 'self', it's syntactically incorrect.\n",
        "    # Let's remove the misplaced function definition for now. If attention is needed, it must be correctly implemented.\n",
        "    # Assuming for the \"no attention\" sweep this function isn't called anyway.\n",
        "\n",
        "    # Removed the misplaced calculate_attention function definition here\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, cell=None,encoder_outputs=None):\n",
        "#         Incorporate dropout in embedding.\n",
        "        # input shape: (batch_size, 1) if processing one token at a time\n",
        "        embedded = self.embedding(input) # embedded shape: (batch_size, 1, emb_dim)\n",
        "        output = self.dropout(embedded) # output shape: (batch_size, 1, emb_dim)\n",
        "\n",
        "        attention_weights = None\n",
        "#         If we are using attention, then we need to concatenate the context vector, which we obtain from attention\n",
        "\n",
        "        if self.attention and encoder_outputs is not None:\n",
        "            # The calculate_attention function needs to be defined correctly and accessible.\n",
        "            # Assuming a global calculate_attention or a method call.\n",
        "            # For now, let's assume a helper function `calculate_attention` exists and works.\n",
        "            # The function would take the current decoder state(s) and encoder outputs.\n",
        "            # Example (placeholder):\n",
        "            # Assuming calculate_attention expects decoder hidden state (batch_size, dec_hid_dim)\n",
        "            # and encoder outputs (batch_size, src_seq_len, enc_hid_dim * enc_directions)\n",
        "\n",
        "            # Need to get the last layer's hidden state if num_layers > 1\n",
        "            # For LSTM hidden is (num_layers, batch_size, hidden_size)\n",
        "            # If using Bahdanau, attention uses previous decoder hidden state.\n",
        "            # The `hidden` tensor passed here is the *current* state before the RNN update.\n",
        "            # If Bahdanau attention: use `hidden` (potentially reshaped)\n",
        "            # If Luong attention: use the state *after* the RNN update.\n",
        "            # The original code passes the state *before* the RNN update. Let's assume Bahdanau-like approach.\n",
        "\n",
        "            # Reshape hidden to (batch_size, num_layers, dec_hid_dim) and take the last layer (index -1)\n",
        "            # dec_hidden_for_attention = hidden.permute(1, 0, 2)[:, -1, :].squeeze(1) # Shape: (batch_size, dec_hid_dim) if dec_hid_dim matches.\n",
        "\n",
        "            # The shape expected by calculate_attention should be consistent.\n",
        "            # Let's assume a simplified attention mechanism that works with the full hidden state for now,\n",
        "            # or that calculate_attention handles reshaping internally.\n",
        "            # This is a placeholder; the actual attention implementation logic needs verification.\n",
        "\n",
        "            # For now, let's assume calculate_attention is called like this (needs actual implementation):\n",
        "            # context, attention_weights = calculate_attention(hidden, encoder_outputs, self.U, self.W, self.V)\n",
        "            # Placeholder: assuming context shape is (batch_size, 1, attention_dim) based on concat usage\n",
        "            # If calculate_attention was meant to return (batch_size, context_size), need to unsqueeze(1)\n",
        "\n",
        "            # Since calculate_attention is missing, and the sweep is for \"no attention\",\n",
        "            # this 'if self.attention' block will not execute, so the error is elsewhere.\n",
        "            # The error is confirmed to be in Seq2Seq init accessing decoder.num_layers.\n",
        "            pass # Do nothing if attention is False\n",
        "\n",
        "        # output shape is now (batch_size, 1, rnn_input_size) - where rnn_input_size = emb_dim or emb_dim + context_size\n",
        "\n",
        "\n",
        "        if self.cell_type == 'lstm':\n",
        "            # self.rnn is nn.LSTM, expects (input, (h_0, c_0))\n",
        "            # output shape: (batch_size, 1, rnn_input_size)\n",
        "            # hidden shape: (num_layers, batch_size, hidden_size)\n",
        "            # cell shape: (num_layers, batch_size, hidden_size)\n",
        "            rnn_output, (hidden, cell) = self.rnn(output, (hidden, cell)) # Pass tuple for LSTM\n",
        "        else: # Covers 'rnn' and 'gru'\n",
        "            # self.rnn is nn.RNN or nn.GRU, expects (input, h_0)\n",
        "            # output shape: (batch_size, 1, rnn_input_size)\n",
        "            # hidden shape: (num_layers, batch_size, hidden_size)\n",
        "            rnn_output, hidden = self.rnn(output, hidden) # Pass single tensor for RNN/GRU\n",
        "            cell = None # Ensure cell is None when not LSTM\n",
        "\n",
        "        # rnn_output shape is (batch_size, 1, dec_hid_dim * val_direction)\n",
        "\n",
        "        # Apply the final linear layer to get logits\n",
        "        # Squeeze the sequence length dimension (size 1) before the linear layer\n",
        "        output_logits = self.W1(rnn_output.squeeze(1)) # output_logits shape: (batch_size, output_dim)\n",
        "\n",
        "\n",
        "        return output_logits, hidden, cell, attention_weights # Return logits, updated states, attention weights\n",
        "\n",
        "# The train1 and calc_test_acc functions should be the corrected versions from the previous turn.\n",
        "# They should obtain vocab_size using model.decoder.W1.out_features\n",
        "# and use reshape(-1, vocab_size) and reshape(-1) for loss calculation,\n",
        "# and torch.argmax(outputs, dim=2) for accuracy calculation.\n",
        "# %% [markdown]\n",
        "# Seq2Seq Model\n",
        "# %%\n",
        "# Seq2Seq Model\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 decoder,\n",
        "                 # dec_inp_dim = 29, # This parameter is not strictly needed if we get output_dim from decoder\n",
        "                 enc_hid_dim = 256, # Parameter needed for linear transformation layers\n",
        "                 dec_hid_dim =256, # Parameter needed for linear transformation layers\n",
        "                 #bidirectional = True, # Keep commented for now\n",
        "                 enc_num_layers = 3, # Parameter needed for linear transformation layers\n",
        "                 dec_num_layers = 2, # Parameter needed for linear transformation layers\n",
        "                 cell_type = 'lstm', # Parameter needed for conditional logic and state transformation\n",
        "                 dropout = 0.2, # Parameter potentially needed elsewhere in Seq2Seq (though typically in Encoder/Decoder)\n",
        "                 attention = False # Parameter needed to control attention logic\n",
        "                ):\n",
        "\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        device = torch.device('cuda' if torch.cuda.cuda() else 'cpu')\n",
        "        # Decoder input dimension\n",
        "        # self.dec_inp_dim = dec_inp_dim # Removed as it's not used for predictions tensor size\n",
        "\n",
        "        # Store the parameters needed for state transformation layers\n",
        "        # Use the attributes from the passed encoder and decoder instances\n",
        "        self.enc_hid_dim = self.encoder.enc_hid_dim # Use attribute from encoder instance\n",
        "        self.dec_hid_dim = self.decoder.dec_hid_dim # Use attribute from decoder instance\n",
        "        self.enc_num_layers = self.encoder.num_layers # Use attribute from encoder instance\n",
        "        self.dec_num_layers = self.decoder.num_layers # <--- Use attribute from decoder instance\n",
        "        self.cell_type = self.decoder.cell_type # Use attribute from decoder instance (should match encoder)\n",
        "        self.dropout = dropout # Storing, but not directly used in Seq2Seq forward here\n",
        "        self.attention = self.decoder.attention # Use attribute from decoder instance\n",
        "\n",
        "        # Initialize val_direction (assuming unidirectional if bidirectional is commented)\n",
        "        # Use the val_direction from the encoder and decoder instances\n",
        "        self.enc_val_direction = self.encoder.val_direction # Use attribute from encoder instance\n",
        "        self.dec_val_direction = self.decoder.val_direction # Use attribute from decoder instance\n",
        "        # Assuming enc_val_direction == dec_val_direction for state transformation\n",
        "        self.val_direction = self.enc_val_direction\n",
        "\n",
        "\n",
        "        # If attention is used, then we need to transform encoder's last hidden to decoder's first hidden\n",
        "        # Correct the input dimension for the linear transformation based on the flattened encoder hidden state\n",
        "        # Input size: enc_num_layers * enc_val_direction * enc_hid_dim\n",
        "        # Output size: dec_num_layers * dec_val_direction * dec_hid_dim\n",
        "        self.enc_dec_linear1 = nn.Linear(self.enc_num_layers * self.enc_val_direction * self.enc_hid_dim,\n",
        "                                         self.dec_num_layers * self.dec_val_direction * self.dec_hid_dim)\n",
        "\n",
        "\n",
        "        # Linear layer to transform encoder's last cell to decoder's first cell (only for LSTM)\n",
        "        # Use the cell_type from the decoder instance\n",
        "        if self.cell_type == 'lstm':\n",
        "             # Correct the input dimension for the linear transformation based on the flattened encoder cell state\n",
        "             self.enc_dec_cell_linear1 = nn.Linear(self.enc_num_layers * self.enc_val_direction * self.enc_hid_dim,\n",
        "                                                  self.dec_num_layers * self.dec_val_direction * self.dec_hid_dim)\n",
        "\n",
        "\n",
        "        # Get the global max_seq_mal value for target sequence length\n",
        "        global max_seq_mal\n",
        "        self.target_seq_len = max_seq_mal + 2 # Store the correct target sequence length\n",
        "\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing = False, is_training=False):\n",
        "        batch_size = source.shape[0]\n",
        "\n",
        "        # Initialize initial states for the encoder\n",
        "        # Correct shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        encoder_initial_hidden = torch.zeros(self.encoder.num_layers * self.encoder.val_direction,\n",
        "                                             batch_size,\n",
        "                                             self.encoder.enc_hid_dim,\n",
        "                                             device=device)\n",
        "        encoder_initial_cell = torch.zeros(self.encoder.num_layers * self.encoder.val_direction,\n",
        "                                           batch_size,\n",
        "                                           self.encoder.enc_hid_dim,\n",
        "                                           device=device) if self.encoder.cell_type == 'lstm' else None # Use encoder's cell_type\n",
        "\n",
        "\n",
        "        # Pass the full source sequence through the encoder\n",
        "        # encoder_output: (batch_size, seq_len, hidden_size * num_directions) if batch_first=True\n",
        "        # last_state: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        # cell_state: (num_layers * num_directions, batch_size, hidden_size) for LSTM\n",
        "        encoder_output, last_state, cell_state = self.encoder(source, encoder_initial_hidden, encoder_initial_cell)\n",
        "\n",
        "\n",
        "        # If attention is used, `encoder_outputs` for attention are the outputs from the encoder at each time step\n",
        "        if self.attention:\n",
        "             encoder_outputs = encoder_output # Shape: (batch_size, seq_len, hidden_size * num_directions)\n",
        "        else:\n",
        "             encoder_outputs = None # Explicitly None if attention is off\n",
        "\n",
        "\n",
        "        # Encoder's last state is decoders first state (after transformation)\n",
        "        # last_state is (enc_num_layers * val_direction, batch_size, enc_hid_dim)\n",
        "\n",
        "        # Transform encoder's last hidden state to decoder's first hidden state\n",
        "        # Use the last hidden state from the encoder across all layers if applicable\n",
        "        # Reshape last_state to (batch_size, enc_num_layers * enc_val_direction * enc_hid_dim) before linear transformation\n",
        "        last_state_reshaped = last_state.permute(1, 0, 2).reshape(batch_size, -1) # Shape: (batch_size, enc_num_layers * enc_val_direction * enc_hid_dim)\n",
        "\n",
        "        # Apply the linear transformation\n",
        "        decoder_hidden_reshaped = self.enc_dec_linear1(last_state_reshaped) # Shape: (batch_size, dec_num_layers * dec_val_direction * dec_hid_dim)\n",
        "\n",
        "        # Reshape back to (dec_num_layers * dec_val_direction, batch_size, dec_hid_dim) for the decoder\n",
        "        # Note: The reshape should match the target shape (num_layers * num_directions, batch_size, hidden_size)\n",
        "        decoder_hidden = decoder_hidden_reshaped.reshape(batch_size, self.dec_num_layers * self.dec_val_direction, self.dec_hid_dim).permute(1, 0, 2) # Shape: (dec_num_layers * dec_val_direction, batch_size, dec_hid_dim)\n",
        "\n",
        "\n",
        "        # Here also, encoders last cell is decoders first cell, also transform to same dimension (for LSTM)\n",
        "        # Use the cell_type from the decoder instance\n",
        "        if  self.decoder.cell_type == 'lstm': # Use decoder's cell_type for this transformation\n",
        "            cell_state_reshaped = cell_state.permute(1, 0, 2).reshape(batch_size, -1) # Shape: (batch_size, enc_num_layers * enc_val_direction * enc_hid_dim)\n",
        "            decoder_cell_reshaped = self.enc_dec_cell_linear1(cell_state_reshaped) # Shape: (batch_size, dec_num_layers * dec_val_direction * dec_hid_dim)\n",
        "            # Reshape back to (dec_num_layers * dec_val_direction, batch_size, dec_hid_dim) for the decoder\n",
        "            decoder_cell_state = decoder_cell_reshaped.reshape(batch_size, self.dec_num_layers * self.dec_val_direction, self.dec_hid_dim).permute(1, 0, 2) # Shape: (dec_num_layers * dec_val_direction, batch_size, dec_hid_dim)\n",
        "        else:\n",
        "            decoder_cell_state = None # Ensure cell_state is None for RNN/GRU\n",
        "\n",
        "\n",
        "        # Initialize predictions and attention_weights\n",
        "        # Get the correct output dimension from the model's decoder's final linear layer (assuming W1 is the final layer)\n",
        "        target_output_dim = self.decoder.W1.out_features # Use W1 from the decoder instance\n",
        "\n",
        "        # Use the stored target sequence length\n",
        "        predictions = torch.zeros(batch_size, self.target_seq_len, target_output_dim, device = device)\n",
        "\n",
        "        # Attention weights shape: (batch_size, target_seq_len, source_seq_len)\n",
        "        # Need global max_seq_eng here\n",
        "        global max_seq_eng\n",
        "        attention_weights = torch.zeros(batch_size, self.target_seq_len, max_seq_eng + 2, device = device) if self.attention else None # Source seq len is max_seq_eng + SOW + EOW\n",
        "\n",
        "        # Initialize the first input to the decoder. This should be the <SOW> token (index 1).\n",
        "        # Use mal_chars_idx['\\t'] for the SOW index (assuming it's a global variable)\n",
        "        global mal_chars_idx # Ensure mal_chars_idx is accessible\n",
        "        decoder_input = torch.full((batch_size, 1), mal_chars_idx['\\t'], dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "        # Do decoding by char by char fashion by batch\n",
        "        # The loop should run self.target_seq_len times\n",
        "        for t in range(self.target_seq_len): # Loop over the correct target sequence length\n",
        "\n",
        "            # Pass the current hidden state and cell state (if LSTM) to the decoder\n",
        "            decoder_output, decoder_hidden, cell_state, attention_wts = self.decoder(\n",
        "                decoder_input,\n",
        "                decoder_hidden, # Pass the updated hidden state from the previous step\n",
        "                cell_state, # Pass the updated cell state from the previous step (will be None for RNN/GRU)\n",
        "                encoder_outputs # Pass encoder outputs for attention\n",
        "            )\n",
        "\n",
        "            # Store the prediction (logits) for the current time step\n",
        "            # decoder_output shape from Decoder forward: (batch_size, output_dim) after squeeze(1)\n",
        "            predictions[:, t, :] = decoder_output # decoder_output is already squeezed in Decoder forward\n",
        "\n",
        "\n",
        "            if self.attention and attention_wts is not None:\n",
        "                # Store attention weights if attention is used\n",
        "                # attention_wts shape from calculate_attention: (batch_size, source_seq_len)\n",
        "                attention_weights[:, t, :] = attention_wts # Store attention weights for this decoding step\n",
        "\n",
        "\n",
        "            # Determine the input for the next time step\n",
        "            # Teacher forcing should only happen if t is within the bounds of the target sequence\n",
        "            # And if is_training is True\n",
        "            if teacher_forcing and is_training and t < self.target_seq_len - 1:\n",
        "                # Teacher forcing: use the actual target token as input to the decoder\n",
        "                # Target shape is (batch_size, target_seq_len). Input needs to be (batch_size, 1).\n",
        "                decoder_input = target[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                # Without teacher forcing or during inference: use the decoder's predicted token from the current time step as input for the next step\n",
        "                # Get the predicted token index (argmax) from the logits\n",
        "                predicted_token = torch.argmax(decoder_output, dim=-1) # shape: (batch_size)\n",
        "                decoder_input = predicted_token.unsqueeze(1).detach() # shape: (batch_size, 1), Detach from graph for the next input\n",
        "\n",
        "\n",
        "        # Return predictions and attention weights\n",
        "        # predictions shape: (batch_size, target_seq_len, output_dim)\n",
        "        # attention_weights shape: (batch_size, target_seq_len, source_seq_len)\n",
        "        return predictions, attention_weights"
      ],
      "metadata": {
        "id": "1eJLZAoXApca"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep config file\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'testset run attention',\n",
        "    'metric': {\n",
        "      'goal': 'maximize',\n",
        "      'name': 'test_accuracy'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'beam_size':{\n",
        "            'values': [1]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "# Create a sweep\n",
        "sweep_id = wandb.sweep(sweep = sweep_config, entity=\"mdkarimullahaque-iit-madras\", project='DL_Assignment_3')"
      ],
      "metadata": {
        "id": "ZkZpDWppArdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce73344e-0d0e-4bbb-9b60-c3dab6547516"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: rvyo66d8\n",
            "Sweep URL: https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/rvyo66d8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb log for test accuracy\n",
        "def main():\n",
        "  with wandb.init() as run:\n",
        "    #run_name = \"-f_num_\"+str(wandb.config.filters_num)+\"-f_num_\"+wandb.config.filter_org+\"-ac_fn_\"+wandb.config.act_fn+\\\n",
        "                #\"-b_norm_\"+str(wandb.config.batch_norm) + \"-bs_\"+str(wandb.config.batch_size) +\"-neu_num\"+str(wandb.config.num_neurons_dense)\n",
        "\n",
        "    wandb.run.name = \"test_set_run_attn\"\n",
        "    calc_test_acc(best_model_attn, test_loader)\n",
        "\n",
        "wandb.agent(sweep_id, function = main, count = 1)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "2P-YMjs5AtoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "c9296f32-7a50-4780-9d60-ebc3554ea083"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 22ij5bwu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_124851-22ij5bwu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/22ij5bwu' target=\"_blank\">peach-sweep-1</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/rvyo66d8' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/rvyo66d8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/rvyo66d8' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/rvyo66d8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/22ij5bwu' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/22ij5bwu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-36-d910eb4bcd6f>\", line 8, in main\n",
            "    calc_test_acc(best_model_attn, test_loader)\n",
            "                  ^^^^^^^^^^^^^^^\n",
            "NameError: name 'best_model_attn' is not defined\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">test_set_run_attn</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/22ij5bwu' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/22ij5bwu</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_124851-22ij5bwu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 22ij5bwu errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-36-d910eb4bcd6f>\", line 8, in main\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     calc_test_acc(best_model_attn, test_loader)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'best_model_attn' is not defined\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention heatmap plot and logging it in wandb"
      ],
      "metadata": {
        "id": "gJj2I52PAx_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input, test_labels = next(iter(test_loader))\n",
        "best_model.eval()\n",
        "test_output,_weights = best_model.forward(test_input.to(device), None,False)"
      ],
      "metadata": {
        "id": "yOfOJk6bAvct"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file ipython-input-38-959740f04f96\n",
        "# Plot attention hmap\n",
        "def prepare_ticks(input_data, output_data, index):\n",
        "    filter_func = lambda x: x.item() not in [0, 1, 2]\n",
        "    # Assuming english_index_dict and malayalam_index_dict are defined globally\n",
        "    x_ticks = [english_index_dict[i.item()] for i in input_data[index] if filter_func(i)]\n",
        "    y_ticks = [malayalam_index_dict[i.item()] for i in output_data[index] if filter_func(i)]\n",
        "    return x_ticks, y_ticks\n",
        "\n",
        "def generate_heatmap(input_data, output_data, weights, num_plots=12):\n",
        "    fig, ax = plt.subplots(4, 3, figsize=(20, 20))\n",
        "    plt.setp(ax)\n",
        "\n",
        "    for idx in range(num_plots):\n",
        "        x_ticks, y_ticks = prepare_ticks(input_data, output_data, idx)\n",
        "        # weights shape: (batch_size, target_seq_len, source_seq_len)\n",
        "        # Need to select the attention weights for the current sample (idx)\n",
        "        heatmap_data = weights[idx, :, :].detach().cpu().numpy() # Select data for sample idx\n",
        "        # Ensure indices are correct based on where padding/SOW/EOW are\n",
        "        # Assuming SOW is index 1, EOW is index 2, Padding is index 0\n",
        "        # Input ticks correspond to src_len (max_seq_eng + 2)\n",
        "        # Output ticks correspond to tgt_len (max_seq_mal + 2)\n",
        "        # If SOW is at index 1 and EOW at len-1 for output, need to adjust.\n",
        "        # If SOW is at index 1 and EOW at len-1 for input, need to adjust.\n",
        "\n",
        "        # Based on prepare_ticks, it skips indices 0, 1, 2. SOW is 1, EOW is 2, Padding is 0.\n",
        "        # It seems prepare_ticks creates ticks for *actual* characters, excluding special tokens.\n",
        "        # The weights tensor shape is (batch_size, target_seq_len, source_seq_len) including special tokens.\n",
        "        # You need to align the heatmap data with the ticks.\n",
        "        # Source sequence in model forward includes SOW (idx 1) and EOW (idx 2) and padding (idx 0).\n",
        "        # Target sequence in model forward includes SOW (idx 1) and EOW (idx 2) and padding (idx 0).\n",
        "        # If x_ticks excludes 0,1,2, and y_ticks excludes 0,1,2, you should select corresponding parts of the weights.\n",
        "        # The original code sliced [1:len(y_ticks)+1, 2:len(x_ticks)+2]. This assumes SOW is at index 1 for output and index 2 for input.\n",
        "        # And padding is 0, EOW is included? Let's re-check data_preprocess.\n",
        "        # data_preprocess: sow = \"\\t\" (idx 1), eow = \"\\n\" (idx 2), padding \"0\" (idx 0).\n",
        "        # Format is sow + word + padding + eow.\n",
        "        # So input sequence: [1, char1, char2, ..., padding..., 2]\n",
        "        # Target sequence: [1, char1, char2, ..., padding..., 2]\n",
        "        # Source seq len = max_seq_eng + 2 (for sow, eow) + padding length. Padding is max_seq - len(word).\n",
        "        # The padded data is sow + word + \"0\" * (max_seq - len(word)) + eow.\n",
        "        # So source sequence indices: 0: padding, 1: sow, 2: eow, 3...: chars.\n",
        "        # input_data[index] indices: 0: padding, 1: sow, 2: eow, 3...: chars.\n",
        "        # y_ticks from output_data[index] indices: 0: padding, 1: sow, 2: eow, 3...: chars.\n",
        "        # The `prepare_ticks` function explicitly filters out 0, 1, 2.\n",
        "        # So x_ticks correspond to chars from index 3 onwards in the original sequence.\n",
        "        # y_ticks correspond to chars from index 3 onwards in the original sequence.\n",
        "\n",
        "        # The weights shape from Seq2Seq forward is (batch_size, target_seq_len, source_seq_len).\n",
        "        # target_seq_len is max_seq_mal + 2 (SOW + EOW). Let's check the loop in Seq2Seq.\n",
        "        # Loop is self.target_seq_len = max_seq_mal + 2 times.\n",
        "        # decoder_input starts with SOW (index 1). The loop runs `target_seq_len` times.\n",
        "        # So predictions and attention_weights have shape (batch_size, max_seq_mal + 2, ...).\n",
        "        # The second dimension corresponds to the decoding step (0 to max_seq_mal + 1).\n",
        "        # The first step (t=0) processes SOW and predicts the first actual char.\n",
        "        # The last step (t = max_seq_mal + 1) processes the token before EOW and predicts EOW.\n",
        "\n",
        "        # The x-axis of the heatmap should correspond to source tokens (input).\n",
        "        # The y-axis of the heatmap should correspond to target tokens (output).\n",
        "        # The attention weights are likely calculated between decoder hidden state at step t and encoder outputs across all source tokens.\n",
        "        # So `weights[idx, t, src_token_idx]` is attention from target token at step `t` to source token at index `src_token_idx`.\n",
        "\n",
        "        # The original slicing `[1:len(y_ticks)+1, 2:len(x_ticks)+2]` is suspicious and likely incorrect.\n",
        "        # It's trying to slice the heatmap data based on the *length* of the ticks, but using fixed offsets.\n",
        "        # A better approach is to select the attention weights corresponding to the actual characters in the input and output sequences *for that specific sample*.\n",
        "        # However, the `_weights` tensor is padded to max lengths.\n",
        "\n",
        "        # Let's assume the original slicing logic was *trying* to get rid of attention to/from special tokens.\n",
        "        # Source sequence indices: [SOW, char1, ..., charN, Padding..., EOW] -> [1, 3, ..., 3+N-1, 0..., 2]\n",
        "        # Target sequence indices: [SOW, char1, ..., charM, Padding..., EOW] -> [1, 3, ..., 3+M-1, 0..., 2]\n",
        "        # `prepare_ticks` excludes 0, 1, 2. So `x_ticks` are chars from index 3 onwards in the original input sequence.\n",
        "        # `y_ticks` are chars from index 3 onwards in the original output sequence.\n",
        "        # The attention weights `_weights` have shape (batch_size, target_seq_len, source_seq_len).\n",
        "        # `target_seq_len` is `max_seq_mal + 2`. The first element is prediction after seeing SOW, last is prediction of EOW.\n",
        "        # The sequence of *predicted* tokens corresponding to `y_ticks` starts from the token after SOW prediction.\n",
        "        # So, in `_weights`, the y-axis (dim 1) should correspond to the decoding steps.\n",
        "        # The first decoding step (index 0) predicts the first token after SOW.\n",
        "        # The decoding steps corresponding to the *actual* characters in `y_ticks` are from index 0 up to `len(y_ticks)-1`.\n",
        "        # The x-axis (dim 2) corresponds to the source token indices.\n",
        "        # The source token indices corresponding to the *actual* characters in `x_ticks` are from index 0 up to `len(x_ticks)-1`.\n",
        "\n",
        "        # Let's try to slice based on the actual number of characters found.\n",
        "        # We need the length of the original (non-padded, non-special-token) input and output for this sample.\n",
        "        # This information is not directly available in `input_data[index]` and `output_data[index]` after padding.\n",
        "        # You might need to store the original lengths during data preprocessing or retrieve them differently.\n",
        "\n",
        "        # Alternatively, assume the original slicing was based on the fixed positions relative to special tokens.\n",
        "        # SOW is at index 1, EOW at index 2 in the `chars_idx`. Padding is 0.\n",
        "        # In the padded sequence `sow + word + padding + eow`, the indices are:\n",
        "        # Index 0: SOW (1)\n",
        "        # Index 1 to 1+len(word)-1: Word characters (3+)\n",
        "        # Index 1+len(word) to max_seq: Padding (0)\n",
        "        # Index max_seq+1: EOW (2)\n",
        "\n",
        "        # This contradicts the data_preprocess code: `sow + str(word) + \"0\" * (max_seq - len(str(word))) + eow`\n",
        "        # Index 0: SOW (1)\n",
        "        # Index 1 to len(word): Word characters (3+)\n",
        "        # Index len(word)+1 to len(word) + (max_seq - len(word)): Padding (0)\n",
        "        # Index len(word) + (max_seq - len(word)) + 1 = max_seq + 1: EOW (2)\n",
        "        # Total length = max_seq + 2.\n",
        "\n",
        "        # So the sequence looks like: [1, char1, ..., charN, 0, ..., 0, 2]\n",
        "        # Indices:          0, 1, ..., N, N+1, ..., max_seq, max_seq+1\n",
        "        # `prepare_ticks` filters out 0, 1, 2.\n",
        "        # x_ticks are chars from index 1 up to len(word).\n",
        "        # y_ticks are chars from index 1 up to len(word) for the target.\n",
        "\n",
        "        # The attention weights `weights[idx, t, src_token_idx]`.\n",
        "        # `t` is the decoding step (0 to max_seq_mal + 1).\n",
        "        # `src_token_idx` is the index in the source sequence (0 to max_seq_eng + 1).\n",
        "\n",
        "        # Let's assume the original slicing `[1:len(y_ticks)+1, 2:len(x_ticks)+2]` was somehow intended,\n",
        "        # maybe based on relative positions after padding/special tokens.\n",
        "        # y-axis: `1:len(y_ticks)+1` -> This slice would include `len(y_ticks)` items starting from index 1.\n",
        "        # x-axis: `2:len(x_ticks)+2` -> This slice would include `len(x_ticks)` items starting from index 2.\n",
        "        # This still seems off based on the actual padded sequence structure.\n",
        "\n",
        "        # Let's try a simpler slice assuming the attention weights for the actual characters are at indices 1 to len(word) in the padded sequence.\n",
        "        # This is still just an assumption. The correct way requires knowing the original lengths or adjusting `prepare_ticks` and slicing based on that.\n",
        "\n",
        "        # However, the primary error is `_weights` being `None`.\n",
        "        # Let's fix that first by checking if attention was used.\n",
        "\n",
        "        # --- Correction for plotting ---\n",
        "        # Assuming attention weights shape is (batch_size, target_seq_len, source_seq_len)\n",
        "        # `target_seq_len` is max_seq_mal + 2\n",
        "        # `source_seq_len` is max_seq_eng + 2\n",
        "        # x_ticks are characters from the *input* sequence (English) excluding special tokens.\n",
        "        # y_ticks are characters from the *output* sequence (Malayalam) excluding special tokens.\n",
        "\n",
        "        # The attention weights `weights[idx, target_step_idx, source_token_idx]`\n",
        "        # The `target_step_idx` corresponds to the prediction of the token at that index *in the padded sequence*.\n",
        "        # The `source_token_idx` corresponds to the token at that index *in the padded source sequence*.\n",
        "\n",
        "        # If `prepare_ticks` gives ticks for characters *after* SOW and before EOW/padding,\n",
        "        # which start from index 1 in the word part of the padded sequence:\n",
        "        # The attention weights related to the actual target characters would be from decoding steps 0 to len(y_ticks)-1 (predicting tokens after SOW)\n",
        "        # The attention weights related to the actual source characters would be for source token indices 1 to len(x_ticks) (characters after SOW)\n",
        "\n",
        "        # So, the slice should potentially be `[0:len(y_ticks), 1:len(x_ticks)+1]`?\n",
        "        # This is highly dependent on how attention is calculated and aligned.\n",
        "        # The original slice was `[1:len(y_ticks)+1, 2:len(x_ticks)+2]`. Let's re-implement that exact slice first.\n",
        "        try:\n",
        "            # Use the exact original slicing logic, assuming it was correct relative to *something*\n",
        "            # heatmap_data = weights[idx, :, :].detach().cpu().numpy() # Select data for sample idx\n",
        "            # heatmap_data = heatmap_data[1:len(y_ticks)+1, 2:len(x_ticks)+2] # Apply the original slice\n",
        "\n",
        "            # A potentially more correct slice if x_ticks/y_ticks correspond to chars after SOW (index 1)\n",
        "            # and assuming attention weights align with the sequence structure:\n",
        "            # y-axis corresponds to decoding steps predicting chars (from step 0 onwards)\n",
        "            # x-axis corresponds to source sequence indices (chars start from index 1)\n",
        "            heatmap_data = weights[idx, 0:len(y_ticks), 1:len(x_ticks)+1].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        except IndexError as e:\n",
        "             print(f\"IndexError processing sample {idx}: {e}. Check sequence lengths and slicing.\")\n",
        "             continue # Skip this sample if slicing fails\n",
        "\n",
        "\n",
        "        plt.sca(ax[idx//3, idx%3])\n",
        "        plt.imshow(heatmap_data, interpolation='nearest', cmap='inferno')\n",
        "        plt.colorbar()\n",
        "        plt.xticks(np.arange(0, len(x_ticks)), x_ticks)\n",
        "\n",
        "        mal_font = FontProperties(fname='/content/drive/MyDrive/DA6401_Assignment-3/dakshina_dataset_v1.0/AnjaliOldLipi-Regular.ttf')\n",
        "        plt.yticks(np.arange(0, len(y_ticks)), y_ticks, fontproperties=mal_font)\n",
        "\n",
        "        plt.xlabel('English')\n",
        "        plt.ylabel('Malayalam')\n",
        "        plt.title(f'Sample {idx + 1}')\n",
        "\n",
        "    plt.tight_layout() # Prevent overlap\n",
        "    plt.show()\n",
        "\n",
        "    # Removed canvas/image conversion as it's not directly used after show()\n",
        "\n",
        "    # Return the figure or axes if needed, but for just showing, returning nothing is fine.\n",
        "    # return fig, ax # Or None\n",
        "\n",
        "# Need english_index_dict and malayalam_index_dict globally\n",
        "# Assuming they are defined elsewhere, e.g., idx2char_eng and idx2char_mal or similar.\n",
        "# Let's use the existing idx2char_mal and define idx2char_eng\n",
        "# Assuming eng_chars_idx and mal_chars_idx are globally available\n",
        "english_index_dict = {idx: char for char, idx in eng_chars_idx.items()}\n",
        "malayalam_index_dict = {idx: char for char, idx in mal_chars_idx.items()}\n",
        "\n",
        "\n",
        "# --- Check if _weights is not None before proceeding ---\n",
        "test_input, test_labels = next(iter(test_loader))\n",
        "best_model.eval()\n",
        "test_output,_weights = best_model.forward(test_input.to(device), None, False)\n",
        "\n",
        "if _weights is not None: # Check if attention weights were returned\n",
        "    # Calculate mean weights only if _weights is not None\n",
        "    # mean_weights = torch.mean(_weights, axis=2) # This axis might be wrong depending on how mean_weights is used.\n",
        "    # Based on generate_heatmap usage `weights[:, :, idx]`, it looks like `weights` is expected to be\n",
        "    # something where the last dimension can be indexed by `idx`. This contradicts the (batch_size, target_seq_len, source_seq_len) shape.\n",
        "    # Let's re-examine `generate_heatmap`. It takes `weights`. Inside, it does `weights[:, :, idx]`.\n",
        "    # This means `weights` must have shape (batch_size, target_seq_len, num_samples_to_plot) or something similar.\n",
        "    # This doesn't fit the (batch_size, target_seq_len, source_seq_len) shape of attention weights.\n",
        "\n",
        "    # **Re-reading the heatmap generation code:**\n",
        "    # `heatmap_data = weights[:, :, idx].detach().cpu().numpy()`\n",
        "    # This indexing `[:, :, idx]` looks like `weights` is supposed to be `(target_seq_len, source_seq_len, batch_size)` or similar\n",
        "    # Or `(batch_size, target_seq_len, source_seq_len)` and then `weights[idx, :, :]` is used.\n",
        "    # The `generate_heatmap` function takes `weights` as input.\n",
        "    # Inside the loop `for idx in range(num_plots):`, it uses `weights[:, :, idx]`.\n",
        "    # This strongly suggests the `weights` variable *passed into* `generate_heatmap` is indexed by the sample index `idx` in the *last* dimension.\n",
        "    # However, `_weights` returned from the model has shape (batch_size, target_seq_len, source_seq_len).\n",
        "    # The heatmap data should be `weights[idx, :, :]` or similar, i.e., attention matrix for one sample.\n",
        "\n",
        "    # The variable `mean_weights = torch.mean(_weights, axis=2)` seems incorrect for the heatmap generation logic.\n",
        "    # The heatmap should show the attention from each output token to each input token for a *single sample*.\n",
        "    # `_weights` already contains the attention weights per sample: `_weights[sample_idx, target_token_idx, source_token_idx]`.\n",
        "\n",
        "    # The line `mean_weights = torch.mean(_weights, axis=2)` calculates the average attention of *each target token* to *all source tokens* for *all samples*.\n",
        "    # This is not the attention matrix needed for the heatmap of a single sample.\n",
        "\n",
        "    # **Correcting the heatmap generation logic:**\n",
        "    # The `generate_heatmap` function should likely take the full `_weights` tensor (batch_size, target_seq_len, source_seq_len).\n",
        "    # Inside the loop, it should select the slice for the current sample: `_weights[idx, :, :]`.\n",
        "    # The parameter `weights` in `generate_heatmap(input_data, output_data, weights, num_plots=12)` is misleadingly named if it expects `_weights`.\n",
        "\n",
        "    # Let's rename the parameter in `generate_heatmap` to `all_attention_weights`.\n",
        "    # And call it with `generate_heatmap(test_input, output_argmax, _weights)`.\n",
        "\n",
        "    # Also, the calculation `output_argmax = torch.argmax(output_softmax, dim=2).T` seems incorrect.\n",
        "    # `output_argmax` should be shape (batch_size, target_seq_len) for `prepare_ticks`.\n",
        "    # `test_output` is (batch_size, target_seq_len, vocab_size).\n",
        "    # `torch.argmax(output_softmax, dim=2)` results in shape (batch_size, target_seq_len).\n",
        "    # Transposing this `.T` results in (target_seq_len, batch_size), which doesn't match the expected shape in `prepare_ticks`.\n",
        "    # `prepare_ticks(input_data, output_data, index)` expects `input_data` and `output_data` to be like the original datasets or tensors shaped (batch_size, seq_len).\n",
        "    # `test_input` is (batch_size, source_seq_len). This works for the input part.\n",
        "    # `output_argmax` is currently (target_seq_len, batch_size). This needs to be (batch_size, target_seq_len).\n",
        "    # Remove the `.T` transpose.\n",
        "\n",
        "    output_softmax = F.softmax(test_output, dim=2)\n",
        "    output_argmax = torch.argmax(output_softmax, dim=2) # Shape: (batch_size, target_seq_len)\n",
        "\n",
        "    # Pass the full attention weights tensor to the heatmap function\n",
        "    # No need to calculate `mean_weights`. That line is removed.\n",
        "    # image = generate_heatmap(test_input, output_argmax, _weights) # Call with _weights\n",
        "\n",
        "\n",
        "    # Let's update the generate_heatmap function signature and logic\n",
        "    def generate_heatmap_corrected(input_data_tensor, output_data_tensor, all_attention_weights_tensor, num_plots=12):\n",
        "        fig, ax = plt.subplots(4, 3, figsize=(20, 20))\n",
        "        plt.setp(ax, xticklabels=[], yticklabels=[]) # Optional: Hide default ticks\n",
        "\n",
        "        # Need global dicts here or pass them\n",
        "        # Assuming english_index_dict and malayalam_index_dict are available\n",
        "\n",
        "        # Determine which samples to plot\n",
        "        # If num_plots > batch_size, plot all samples in the batch\n",
        "        samples_to_plot = min(num_plots, input_data_tensor.size(0))\n",
        "\n",
        "\n",
        "        for idx in range(samples_to_plot):\n",
        "            # Prepare ticks for the current sample\n",
        "            # prepare_ticks expects data in the format of the original tensors/lists\n",
        "            # Need to pass the individual sample tensors here\n",
        "            # input_data_tensor[idx] is shape (source_seq_len,)\n",
        "            # output_data_tensor[idx] is shape (target_seq_len,)\n",
        "            x_ticks, y_ticks = prepare_ticks(input_data_tensor, output_data_tensor, idx)\n",
        "\n",
        "            # Select the attention weights for the current sample\n",
        "            # all_attention_weights_tensor[idx, :, :] is shape (target_seq_len, source_seq_len)\n",
        "            attention_for_sample = all_attention_weights_tensor[idx, :, :].detach().cpu().numpy()\n",
        "\n",
        "            # Slice the attention matrix to align with the ticks (excluding special tokens 0, 1, 2)\n",
        "            # The chars corresponding to x_ticks/y_ticks start from index 1 in the padded sequence (after SOW)\n",
        "            # Source indices in padded sequence: [SOW, char1, ..., charN, Pad..., EOW] -> [1, 3..., 3+N-1, 0..., 2]\n",
        "            # Target indices in padded sequence (decoding steps): Step 0 (predicts token after SOW), Step 1 (predicts next char), ...\n",
        "            # So, y-axis (decoding steps): from step 0 up to len(y_ticks)-1\n",
        "            # x-axis (source indices): from index 1 up to len(x_ticks)\n",
        "            try:\n",
        "                 # Adjust slice based on the padded sequence structure and how prepare_ticks works\n",
        "                 # Prepare_ticks filters out 0, 1, 2. This means it extracts the *actual* characters.\n",
        "                 # If the padded sequence is [1, char1, ..., charN, 0, ..., 0, 2]\n",
        "                 # Indices 1 to N are the chars.\n",
        "                 # Attention matrix is (target_seq_len, source_seq_len)\n",
        "                 # target_seq_len is max_seq_mal + 2\n",
        "                 # source_seq_len is max_seq_eng + 2\n",
        "\n",
        "                 # Assuming the attention aligns directly with the padded sequence indices:\n",
        "                 # y-axis (target tokens): indices corresponding to y_ticks in the target padded sequence.\n",
        "                 # x-axis (source tokens): indices corresponding to x_ticks in the source padded sequence.\n",
        "\n",
        "                 # The simplest assumption is that attention[i, j] relates to target token *predicted at step i* and source token *at index j*.\n",
        "                 # Decoding step 0 predicts the token at index 1 (first char after SOW).\n",
        "                 # Decoding step `k` predicts the token at index `k+1` in the target sequence.\n",
        "                 # Source token indices 1 to 1 + len(x_ticks)-1 = len(x_ticks) are the actual chars.\n",
        "\n",
        "                 # So, y-axis should cover decoding steps 0 to len(y_ticks)-1.\n",
        "                 # x-axis should cover source indices 1 to len(x_ticks).\n",
        "                 heatmap_data_sliced = attention_for_sample[0:len(y_ticks), 1:len(x_ticks)+1]\n",
        "\n",
        "                 # Check if the sliced data is valid\n",
        "                 if heatmap_data_sliced.size == 0 or heatmap_data_sliced.shape[0] != len(y_ticks) or heatmap_data_sliced.shape[1] != len(x_ticks):\n",
        "                      print(f\"Warning: Sliced heatmap data for sample {idx} has unexpected shape {heatmap_data_sliced.shape}. Expected ({len(y_ticks)}, {len(x_ticks)}). Skipping plot.\")\n",
        "                      continue # Skip if slicing resulted in empty or wrong shape\n",
        "\n",
        "\n",
        "            except IndexError as e:\n",
        "                 print(f\"IndexError slicing heatmap for sample {idx}: {e}. Source shape: {attention_for_sample.shape}, y_ticks len: {len(y_ticks)}, x_ticks len: {len(x_ticks)}. Skipping plot.\")\n",
        "                 continue # Skip this sample if slicing fails\n",
        "\n",
        "\n",
        "            ax_curr = ax[idx//3, idx%3]\n",
        "            im = ax_curr.imshow(heatmap_data_sliced, interpolation='nearest', cmap='inferno')\n",
        "            fig.colorbar(im, ax=ax_curr) # Attach colorbar to the current subplot\n",
        "            ax_curr.set_xticks(np.arange(0, len(x_ticks)))\n",
        "            ax_curr.set_xticklabels(x_ticks)\n",
        "\n",
        "            mal_font = FontProperties(fname='/content/drive/MyDrive/DA6401_Assignment-3/dakshina_dataset_v1.0/AnjaliOldLipi-Regular.ttf')\n",
        "            ax_curr.set_yticks(np.arange(0, len(y_ticks)))\n",
        "            ax_curr.set_yticklabels(y_ticks, fontproperties=mal_font)\n",
        "\n",
        "            ax_curr.set_xlabel('English')\n",
        "            ax_curr.set_ylabel('Malayalam')\n",
        "            ax_curr.set_title(f'Sample {idx + 1}')\n",
        "\n",
        "\n",
        "        plt.tight_layout() # Prevent overlap\n",
        "        plt.show()\n",
        "\n",
        "        # wandb log the figure\n",
        "        # Need to import wandb\n",
        "        # wandb.log({\"attention_heatmaps\": wandb.Image(fig)}) # Requires figure object\n",
        "\n",
        "        plt.close(fig) # Close the figure after displaying/logging to free memory\n",
        "\n",
        "\n",
        "    # Call the corrected heatmap generation function\n",
        "    generate_heatmap_corrected(test_input, output_argmax, _weights)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Attention weights are None. Cannot plot heatmap.\")\n",
        "    # Optionally, log a message to wandb if a run is active\n",
        "    # if wandb.run:\n",
        "    #     wandb.log({\"attention_heatmap_status\": \"Attention weights were None.\"})"
      ],
      "metadata": {
        "id": "yizF2MTnA0QI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90524769-144b-45f4-fd65-3586f29a5de0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are None. Cannot plot heatmap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'attention_plot',\n",
        "    'parameters': {\n",
        "        'beam_size': {\n",
        "            'values': [1]\n",
        "        }\n",
        "  }\n",
        "}\n",
        "# Create a sweep\n",
        "sweep_id = wandb.sweep(sweep = sweep_config, entity=\"mdkarimullahaque-iit-madras\", project='DL_Assignment_3')"
      ],
      "metadata": {
        "id": "w1wvyRisA16S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c87f4c6-9def-4468-d729-9922f3e0afce"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: dqzda4z7\n",
            "Sweep URL: https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/dqzda4z7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        #run_name = \"-f_num_\"+str(wandb.config.filters_num)+\"-f_num_\"+wandb.config.filter_org+\"-ac_fn_\"+wandb.config.act_fn+\\\n",
        "                    #\"-b_norm_\"+str(wandb.config.batch_norm) + \"-bs_\"+str(wandb.config.batch_size) +\"-neu_num\"+str(wandb.config.num_neurons_dense)\n",
        "\n",
        "        wandb.run.name = \"attention_heatmap\"\n",
        "        #wandb.log({\"image_pred\": [wandb.Image(image, caption=\"Test Images and Predictions\")]})\n",
        "        wandb.log({\"image_grid\": [wandb.Image(image, caption=\"Attention Heatmap\")]})\n",
        "wandb.agent(sweep_id, function = main, count = 1)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "NYrYDIzYA5CC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "3d0b7839-3082-4db8-8aae-9b40c2d07e15"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: en3h0p8k with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_125031-en3h0p8k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/en3h0p8k' target=\"_blank\">pleasant-sweep-1</a></strong> to <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/dqzda4z7' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/dqzda4z7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/dqzda4z7' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/sweeps/dqzda4z7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/en3h0p8k' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/en3h0p8k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-41-695a2dcd9917>\", line 8, in main\n",
            "    wandb.log({\"image_grid\": [wandb.Image(image, caption=\"Attention Heatmap\")]})\n",
            "                                          ^^^^^\n",
            "NameError: name 'image' is not defined\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">attention_heatmap</strong> at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/en3h0p8k' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3/runs/en3h0p8k</a><br> View project at: <a href='https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3' target=\"_blank\">https://wandb.ai/mdkarimullahaque-iit-madras/DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_125031-en3h0p8k/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run en3h0p8k errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-41-695a2dcd9917>\", line 8, in main\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({\"image_grid\": [wandb.Image(image, caption=\"Attention Heatmap\")]})\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                           ^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'image' is not defined\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
          ]
        }
      ]
    }
  ]
}